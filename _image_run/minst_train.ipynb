{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dfb7dc14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 23400\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import argparse\n",
    "import sys\n",
    "import torch\n",
    "\n",
    "# 检查并移除 Jupyter 传递的参数\n",
    "if any('--f=' in arg for arg in sys.argv):\n",
    "    # 移除包含 '--f=' 的所有参数\n",
    "    sys.argv = [arg for arg in sys.argv if not arg.startswith('--f=')]\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(description=\"Train a Mixture-of-Experts model.\")\n",
    "    parser.add_argument(\"--epochs\", type=int, default=50, help=\"Number of epochs to train.\")\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=128, help=\"Batch size for training.\")\n",
    "    parser.add_argument(\"--input_size\", type=int, default=20, help=\"Input size for the moe.\")\n",
    "    parser.add_argument(\"--num_experts\", type=int, default=2, help=\"Number of experts.\")\n",
    "    parser.add_argument(\"--hidden_size\", type=int, default=10, help=\"Hidden size for the moe.\")\n",
    "    parser.add_argument(\"--depth\", type=int, default=2, help=\"Depth of the experts.\")\n",
    "    parser.add_argument(\"--output_size\", type=int, default=10, help=\"Output size for the experts.\")\n",
    "    parser.add_argument(\"--activation\", type=str, default=\"tanh\", help=\"Activation function for the moe.\")\n",
    "    parser.add_argument(\"--k\", type=int, default=1, help=\"Top-k experts to use.\")\n",
    "    parser.add_argument(\"--loss_coef\", type=float, default=0, help=\"Coefficient for the loss.\")\n",
    "    parser.add_argument(\"--smooth_steps\",type=int,default=1,help=\"number of steps for smooth mode\")\n",
    "    parser.add_argument(\"--smooth_lb\",type=int,default=200,help=\"number lower bound of steps for smooth mode\")\n",
    "    parser.add_argument(\"--seed\",type=int,default=1234) #1234\n",
    "    return parser.parse_args()\n",
    "args=parse_args()\n",
    "torch.manual_seed(args.seed)\n",
    "iteration= (60000 // args.batch_size) * args.epochs\n",
    "print(\"iteration:\",iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "05d21520",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在下载/加载 MNIST 训练数据集...\n",
      "数据集加载完成。\n",
      "\n",
      "--- 第一批数据结果 ---\n",
      "图片张量形状 (Batch, C, H, W): torch.Size([4, 1, 28, 28])\n",
      "标签张量形状 (Batch): torch.Size([4])\n",
      "\n",
      "--- 图像 1 ---\n",
      "  标签: 6\n",
      "  部分像素值 (前5x5):\n",
      "tensor([[-0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "        [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "        [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "        [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "        [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242]])\n",
      "--------------------\n",
      "--- 图像 2 ---\n",
      "  标签: 6\n",
      "  部分像素值 (前5x5):\n",
      "tensor([[-0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "        [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "        [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "        [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "        [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242]])\n",
      "--------------------\n",
      "--- 图像 3 ---\n",
      "  标签: 3\n",
      "  部分像素值 (前5x5):\n",
      "tensor([[-0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "        [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "        [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "        [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "        [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242]])\n",
      "--------------------\n",
      "--- 图像 4 ---\n",
      "  标签: 8\n",
      "  部分像素值 (前5x5):\n",
      "tensor([[-0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "        [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "        [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "        [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "        [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242]])\n",
      "--------------------\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x300 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt # 用于可视化，可选\n",
    "\n",
    "# 1. 定义数据预处理步骤\n",
    "# 转换为 Tensor，并进行标准化（这是深度学习的标准步骤）\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    # MNIST的均值和标准差，用于将像素值归一化到 [-1, 1] 左右\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "# 2. 下载并加载数据集\n",
    "# root='.' 表示下载到当前文件夹\n",
    "# train=True 表示下载训练集 (共60000张)\n",
    "# download=True 表示如果本地没有，则下载\n",
    "print(\"正在下载/加载 MNIST 训练数据集...\")\n",
    "train_dataset = torchvision.datasets.MNIST(root='/home/zhy/Zhou/mixture_of_experts/_image_run/MINST/', \n",
    "                                           train=True, \n",
    "                                           transform=transform, \n",
    "                                           download=True)\n",
    "print(\"数据集加载完成。\")\n",
    "\n",
    "# 3. 使用 DataLoader 载入数据\n",
    "# 设置一个较小的 batch_size 方便查看结果\n",
    "batch_size = 4\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=True)\n",
    "\n",
    "# 4. 取出并打印第一批数据\n",
    "# 迭代器next()方法取出第一批数据\n",
    "data_iter = iter(train_loader)\n",
    "images, labels = next(data_iter)\n",
    "\n",
    "print(\"\\n--- 第一批数据结果 ---\")\n",
    "print(f\"图片张量形状 (Batch, C, H, W): {images.shape}\")\n",
    "print(f\"标签张量形状 (Batch): {labels.shape}\\n\")\n",
    "\n",
    "# 5. 打印前4张图片的信息\n",
    "for i in range(batch_size):\n",
    "    print(f\"--- 图像 {i+1} ---\")\n",
    "    \n",
    "    # 打印标签\n",
    "    print(f\"  标签: {labels[i].item()}\")\n",
    "    \n",
    "    # 打印部分像素值 (张量切片)\n",
    "    # 形状是 (1, 28, 28)，这里打印第0通道的前5行5列的像素值\n",
    "    print(\"  部分像素值 (前5x5):\")\n",
    "    print(images[i, 0, :5, :5])\n",
    "    print(\"-\" * 20)\n",
    "# （接上面的代码，在第5步之后运行）\n",
    "fig, axes = plt.subplots(1, batch_size, figsize=(10, 3))\n",
    "for i in range(batch_size):\n",
    "    # 移除标准化并转为Numpy数组\n",
    "    img = images[i].numpy().transpose((1, 2, 0)) # 将(C, H, W)转为(H, W, C)\n",
    "    \n",
    "    # 因为图像是归一化过的，这里需要简单的反归一化来正确显示\n",
    "    # 仅为了显示效果，我们直接裁剪到有效范围\n",
    "    axes[i].imshow(img.squeeze(), cmap='gray') \n",
    "    axes[i].set_title(f\"Label: {labels[i].item()}\")\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7751b15a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用的设备: cuda:7\n",
      "=== Trainable parameters ===\n",
      "Total trainable params: 1713\n",
      "\n",
      "---  模型结构 ---\n",
      "MoE_vision(\n",
      "  (activation): Sigmoid()\n",
      "  (trans_layer): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=20, bias=True)\n",
      "    (1): ReLU()\n",
      "  )\n",
      "  (moe): ModuleList(\n",
      "    (0): MoE(\n",
      "      (experts): ModuleList(\n",
      "        (0-1): 2 x Expert(\n",
      "          (activation): Sigmoid()\n",
      "          (net): ModuleList(\n",
      "            (0): Linear(in_features=20, out_features=10, bias=True)\n",
      "            (1): Linear(in_features=10, out_features=10, bias=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (gating_network): Gate_fcnn(\n",
      "        (fc): Sequential(\n",
      "          (0): Linear(in_features=20, out_features=2, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=2, out_features=2, bias=True)\n",
      "        )\n",
      "        (noisy): Linear(in_features=20, out_features=2, bias=True)\n",
      "        (softplus): Softplus(beta=1.0, threshold=20.0)\n",
      "      )\n",
      "    )\n",
      "    (1-2): 2 x MoE(\n",
      "      (experts): ModuleList(\n",
      "        (0-1): 2 x Expert(\n",
      "          (activation): Sigmoid()\n",
      "          (net): ModuleList(\n",
      "            (0-1): 2 x Linear(in_features=10, out_features=10, bias=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (gating_network): Gate_fcnn(\n",
      "        (fc): Sequential(\n",
      "          (0): Linear(in_features=10, out_features=2, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=2, out_features=2, bias=True)\n",
      "        )\n",
      "        (noisy): Linear(in_features=10, out_features=2, bias=True)\n",
      "        (softplus): Softplus(beta=1.0, threshold=20.0)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "输入尺寸: torch.Size([64, 1, 28, 28])\n",
      "输出尺寸: torch.Size([64, 10])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from moe import *\n",
    "device = torch.device(\"cuda:7\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"使用的设备: {device}\")\n",
    "net = MoE_vision(args.input_size, args.num_experts, args.hidden_size, args.depth, args.output_size, args.k, args.loss_coef,activation=nn.Sigmoid()).to(device)\n",
    "# 打印模型结构\n",
    "print(\"---  模型结构 ---\")\n",
    "print(net)\n",
    "\n",
    "# 检查输入/输出尺寸\n",
    "# 假设批量大小为 64，输入是 (64, 1, 28, 28)\n",
    "input_tensor = torch.randn(64, 1, 28, 28).to(device) \n",
    "output , loss= net(input_tensor)\n",
    "print(f\"\\n输入尺寸: {input_tensor.shape}\")\n",
    "print(f\"输出尺寸: {output.shape}\") \n",
    "# 预期输出: torch.Size([64, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8526eb28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 开始训练 ---\n",
      "tau2 before smooth: tensor([0.1000], device='cuda:7', grad_fn=<ExpBackward0>)\n",
      "tau2 before smooth: tensor([0.1000], device='cuda:7', grad_fn=<ExpBackward0>)\n",
      "tau2 before smooth: tensor([0.1000], device='cuda:7', grad_fn=<ExpBackward0>)\n",
      "[   1,   100] loss: 2.302, aux_loss: 0.000\n",
      "reg: 0.141346275806427\n",
      "[   1,   200] loss: 2.293, aux_loss: 0.000\n",
      "reg: 0.06642487645149231\n",
      "[   1,   300] loss: 2.155, aux_loss: 0.000\n",
      "reg: 0.04116752743721008\n",
      "[   1,   400] loss: 2.111, aux_loss: 0.000\n",
      "reg: 0.027791567146778107\n",
      "tau2 before smooth: tensor([0.3763], device='cuda:7', grad_fn=<ExpBackward0>)\n",
      "tau2 before smooth: tensor([0.3728], device='cuda:7', grad_fn=<ExpBackward0>)\n",
      "tau2 before smooth: tensor([0.3567], device='cuda:7', grad_fn=<ExpBackward0>)\n",
      "[   2,   100] loss: 2.142, aux_loss: 0.000\n",
      "reg: 0.07020844519138336\n",
      "[   2,   200] loss: 2.077, aux_loss: 0.000\n",
      "reg: 0.03859524056315422\n",
      "[   2,   300] loss: 2.055, aux_loss: 0.000\n",
      "reg: 0.025925468653440475\n",
      "[   2,   400] loss: 2.047, aux_loss: 0.000\n",
      "reg: 0.019075680524110794\n",
      "tau2 before smooth: tensor([0.4432], device='cuda:7', grad_fn=<ExpBackward0>)\n",
      "tau2 before smooth: tensor([0.4217], device='cuda:7', grad_fn=<ExpBackward0>)\n",
      "tau2 before smooth: tensor([0.4365], device='cuda:7', grad_fn=<ExpBackward0>)\n",
      "[   3,   100] loss: 2.061, aux_loss: 0.000\n",
      "reg: 0.062043696641922\n",
      "[   3,   200] loss: 2.026, aux_loss: 0.000\n",
      "reg: 0.03368397429585457\n",
      "[   3,   300] loss: 2.016, aux_loss: 0.000\n",
      "reg: 0.022466860711574554\n",
      "[   3,   400] loss: 2.008, aux_loss: 0.000\n",
      "reg: 0.016575682908296585\n",
      "tau2 before smooth: tensor([0.4728], device='cuda:7', grad_fn=<ExpBackward0>)\n",
      "tau2 before smooth: tensor([0.4522], device='cuda:7', grad_fn=<ExpBackward0>)\n",
      "tau2 before smooth: tensor([0.4686], device='cuda:7', grad_fn=<ExpBackward0>)\n",
      "[   4,   100] loss: 2.019, aux_loss: 0.000\n",
      "reg: 0.05682195723056793\n",
      "[   4,   200] loss: 1.995, aux_loss: 0.000\n",
      "reg: 0.030394721776247025\n",
      "[   4,   300] loss: 1.997, aux_loss: 0.000\n",
      "reg: 0.020355628803372383\n",
      "[   4,   400] loss: 1.986, aux_loss: 0.000\n",
      "reg: 0.015102459117770195\n",
      "tau2 before smooth: tensor([0.4895], device='cuda:7', grad_fn=<ExpBackward0>)\n",
      "tau2 before smooth: tensor([0.4725], device='cuda:7', grad_fn=<ExpBackward0>)\n",
      "tau2 before smooth: tensor([0.4957], device='cuda:7', grad_fn=<ExpBackward0>)\n",
      "[   5,   100] loss: 2.008, aux_loss: 0.000\n",
      "reg: 0.05294717848300934\n",
      "[   5,   200] loss: 1.987, aux_loss: 0.000\n",
      "reg: 0.028077084571123123\n",
      "[   5,   300] loss: 1.975, aux_loss: 0.000\n",
      "reg: 0.019002258777618408\n",
      "[   5,   400] loss: 1.975, aux_loss: 0.000\n",
      "reg: 0.014174377545714378\n",
      "tau2 before smooth: tensor([0.5004], device='cuda:7', grad_fn=<ExpBackward0>)\n",
      "tau2 before smooth: tensor([0.4883], device='cuda:7', grad_fn=<ExpBackward0>)\n",
      "tau2 before smooth: tensor([0.5137], device='cuda:7', grad_fn=<ExpBackward0>)\n",
      "[   6,   100] loss: 1.970, aux_loss: 0.000\n",
      "reg: 0.048310842365026474\n",
      "[   6,   200] loss: 1.972, aux_loss: 0.000\n",
      "reg: 0.026440244168043137\n",
      "[   6,   300] loss: 1.962, aux_loss: 0.000\n",
      "reg: 0.017901087179780006\n",
      "[   6,   400] loss: 1.970, aux_loss: 0.000\n",
      "reg: 0.013369403779506683\n",
      "tau2 before smooth: tensor([0.5171], device='cuda:7', grad_fn=<ExpBackward0>)\n",
      "tau2 before smooth: tensor([0.5027], device='cuda:7', grad_fn=<ExpBackward0>)\n",
      "tau2 before smooth: tensor([0.5256], device='cuda:7', grad_fn=<ExpBackward0>)\n",
      "[   7,   100] loss: 2.015, aux_loss: 0.000\n",
      "reg: 0.04463528096675873\n",
      "[   7,   200] loss: 1.953, aux_loss: 0.000\n",
      "reg: 0.02470039390027523\n",
      "[   7,   300] loss: 1.955, aux_loss: 0.000\n",
      "reg: 0.016945846378803253\n",
      "[   7,   400] loss: 1.951, aux_loss: 0.000\n",
      "reg: 0.012756405398249626\n",
      "tau2 before smooth: tensor([0.5311], device='cuda:7', grad_fn=<ExpBackward0>)\n",
      "tau2 before smooth: tensor([0.5131], device='cuda:7', grad_fn=<ExpBackward0>)\n",
      "tau2 before smooth: tensor([0.5355], device='cuda:7', grad_fn=<ExpBackward0>)\n",
      "[   8,   100] loss: 1.955, aux_loss: 0.000\n",
      "reg: 0.044020332396030426\n",
      "[   8,   200] loss: 1.934, aux_loss: 0.000\n",
      "reg: 0.024298090487718582\n",
      "[   8,   300] loss: 1.937, aux_loss: 0.000\n",
      "reg: 0.016614433377981186\n",
      "[   8,   400] loss: 1.921, aux_loss: 0.000\n",
      "reg: 0.012524036690592766\n",
      "tau2 before smooth: tensor([0.5366], device='cuda:7', grad_fn=<ExpBackward0>)\n",
      "tau2 before smooth: tensor([0.5192], device='cuda:7', grad_fn=<ExpBackward0>)\n",
      "tau2 before smooth: tensor([0.5381], device='cuda:7', grad_fn=<ExpBackward0>)\n",
      "[   9,   100] loss: 1.931, aux_loss: 0.000\n",
      "reg: 0.04293901473283768\n",
      "[   9,   200] loss: 1.907, aux_loss: 0.000\n",
      "reg: 0.023851769044995308\n",
      "[   9,   300] loss: 1.908, aux_loss: 0.000\n",
      "reg: 0.01632646843791008\n",
      "[   9,   400] loss: 1.903, aux_loss: 0.000\n",
      "reg: 0.012275349348783493\n",
      "tau2 before smooth: tensor([0.5382], device='cuda:7', grad_fn=<ExpBackward0>)\n",
      "tau2 before smooth: tensor([0.5291], device='cuda:7', grad_fn=<ExpBackward0>)\n",
      "tau2 before smooth: tensor([0.5426], device='cuda:7', grad_fn=<ExpBackward0>)\n",
      "[  10,   100] loss: 1.923, aux_loss: 0.000\n",
      "reg: 0.044487278908491135\n",
      "[  10,   200] loss: 1.902, aux_loss: 0.000\n",
      "reg: 0.024158470332622528\n",
      "[  10,   300] loss: 1.901, aux_loss: 0.000\n",
      "reg: 0.016410332173109055\n",
      "[  10,   400] loss: 1.897, aux_loss: 0.000\n",
      "reg: 0.01231314241886139\n",
      "tau2 before smooth: tensor([0.5369], device='cuda:7', grad_fn=<ExpBackward0>)\n",
      "tau2 before smooth: tensor([0.5234], device='cuda:7', grad_fn=<ExpBackward0>)\n",
      "tau2 before smooth: tensor([0.5484], device='cuda:7', grad_fn=<ExpBackward0>)\n",
      "[  11,   100] loss: 1.926, aux_loss: 0.000\n",
      "reg: 0.04244749993085861\n",
      "[  11,   200] loss: 1.899, aux_loss: 0.000\n",
      "reg: 0.02338412031531334\n",
      "[  11,   300] loss: 1.895, aux_loss: 0.000\n",
      "reg: 0.016036689281463623\n",
      "[  11,   400] loss: 1.887, aux_loss: 0.000\n",
      "reg: 0.012089956551790237\n",
      "tau2 before smooth: tensor([0.5360], device='cuda:7', grad_fn=<ExpBackward0>)\n",
      "tau2 before smooth: tensor([0.5369], device='cuda:7', grad_fn=<ExpBackward0>)\n",
      "tau2 before smooth: tensor([0.5487], device='cuda:7', grad_fn=<ExpBackward0>)\n",
      "[  12,   100] loss: 1.901, aux_loss: 0.000\n",
      "reg: 0.04358270391821861\n",
      "[  12,   200] loss: 1.885, aux_loss: 0.000\n",
      "reg: 0.023496173322200775\n",
      "[  12,   300] loss: 1.886, aux_loss: 0.000\n",
      "reg: 0.015963325276970863\n",
      "[  12,   400] loss: 1.879, aux_loss: 0.000\n",
      "reg: 0.011982030235230923\n",
      "tau2 before smooth: tensor([0.5402], device='cuda:7', grad_fn=<ExpBackward0>)\n",
      "tau2 before smooth: tensor([0.5402], device='cuda:7', grad_fn=<ExpBackward0>)\n",
      "tau2 before smooth: tensor([0.5498], device='cuda:7', grad_fn=<ExpBackward0>)\n",
      "[  13,   100] loss: 1.976, aux_loss: 0.000\n",
      "reg: 0.04326314851641655\n",
      "[  13,   200] loss: 1.880, aux_loss: 0.000\n",
      "reg: 0.023151595145463943\n",
      "[  13,   300] loss: 1.883, aux_loss: 0.000\n",
      "reg: 0.015800543129444122\n",
      "[  13,   400] loss: 1.886, aux_loss: 0.000\n",
      "reg: 0.0118657685816288\n",
      "tau2 before smooth: tensor([0.5465], device='cuda:7', grad_fn=<ExpBackward0>)\n",
      "tau2 before smooth: tensor([0.5340], device='cuda:7', grad_fn=<ExpBackward0>)\n",
      "tau2 before smooth: tensor([0.5575], device='cuda:7', grad_fn=<ExpBackward0>)\n",
      "[  14,   100] loss: 1.896, aux_loss: 0.000\n",
      "reg: 0.042258962988853455\n",
      "[  14,   200] loss: 1.871, aux_loss: 0.000\n",
      "reg: 0.023100141435861588\n",
      "[  14,   300] loss: 1.876, aux_loss: 0.000\n",
      "reg: 0.015729697421193123\n",
      "[  14,   400] loss: 1.864, aux_loss: 0.000\n",
      "reg: 0.011800800450146198\n",
      "tau2 before smooth: tensor([0.5397], device='cuda:7', grad_fn=<ExpBackward0>)\n",
      "tau2 before smooth: tensor([0.5507], device='cuda:7', grad_fn=<ExpBackward0>)\n",
      "tau2 before smooth: tensor([0.5516], device='cuda:7', grad_fn=<ExpBackward0>)\n",
      "[  15,   100] loss: 1.912, aux_loss: 0.000\n",
      "reg: 0.040619608014822006\n",
      "[  15,   200] loss: 1.869, aux_loss: 0.000\n",
      "reg: 0.0239472147077322\n",
      "[  15,   300] loss: 1.861, aux_loss: 0.000\n",
      "reg: 0.01689731329679489\n",
      "[  15,   400] loss: 1.858, aux_loss: 0.000\n",
      "reg: 0.012935184873640537\n",
      "tau2 before smooth: tensor([0.4685], device='cuda:7', grad_fn=<ExpBackward0>)\n",
      "tau2 before smooth: tensor([0.5516], device='cuda:7', grad_fn=<ExpBackward0>)\n",
      "tau2 before smooth: tensor([0.5553], device='cuda:7', grad_fn=<ExpBackward0>)\n",
      "[  16,   100] loss: 1.883, aux_loss: 0.000\n",
      "reg: 0.043842438608407974\n",
      "[  16,   200] loss: 1.860, aux_loss: 0.000\n",
      "reg: 0.024798989295959473\n",
      "[  16,   300] loss: 1.861, aux_loss: 0.000\n",
      "reg: 0.01717717945575714\n",
      "[  16,   400] loss: 1.853, aux_loss: 0.000\n",
      "reg: 0.013033386319875717\n",
      "tau2 before smooth: tensor([0.4668], device='cuda:7', grad_fn=<ExpBackward0>)\n",
      "tau2 before smooth: tensor([0.5516], device='cuda:7', grad_fn=<ExpBackward0>)\n",
      "tau2 before smooth: tensor([0.5564], device='cuda:7', grad_fn=<ExpBackward0>)\n",
      "[  17,   100] loss: 1.877, aux_loss: 0.000\n",
      "reg: 0.04771685600280762\n",
      "[  17,   200] loss: 1.853, aux_loss: 0.000\n",
      "reg: 0.02574218064546585\n",
      "[  17,   300] loss: 1.858, aux_loss: 0.000\n",
      "reg: 0.01760629191994667\n",
      "[  17,   400] loss: 1.846, aux_loss: 0.000\n",
      "reg: 0.013260631822049618\n",
      "tau2 before smooth: tensor([0.4801], device='cuda:7', grad_fn=<ExpBackward0>)\n",
      "tau2 before smooth: tensor([0.5203], device='cuda:7', grad_fn=<ExpBackward0>)\n",
      "tau2 before smooth: tensor([0.5567], device='cuda:7', grad_fn=<ExpBackward0>)\n",
      "[  18,   100] loss: 1.885, aux_loss: 0.000\n",
      "reg: 0.047699280083179474\n",
      "[  18,   200] loss: 1.860, aux_loss: 0.000\n",
      "reg: 0.025638818740844727\n",
      "[  18,   300] loss: 1.847, aux_loss: 0.000\n",
      "reg: 0.01736750826239586\n",
      "[  18,   400] loss: 1.847, aux_loss: 0.000\n",
      "reg: 0.013026225380599499\n",
      "tau2 before smooth: tensor([0.4821], device='cuda:7', grad_fn=<ExpBackward0>)\n",
      "tau2 before smooth: tensor([0.5346], device='cuda:7', grad_fn=<ExpBackward0>)\n",
      "tau2 before smooth: tensor([0.5551], device='cuda:7', grad_fn=<ExpBackward0>)\n",
      "[  19,   100] loss: 1.876, aux_loss: 0.000\n",
      "reg: 0.04385053738951683\n",
      "[  19,   200] loss: 1.845, aux_loss: 0.000\n",
      "reg: 0.024308091029524803\n",
      "[  19,   300] loss: 1.841, aux_loss: 0.000\n",
      "reg: 0.016660908237099648\n",
      "[  19,   400] loss: 1.846, aux_loss: 0.000\n",
      "reg: 0.012563866563141346\n",
      "tau2 before smooth: tensor([0.4966], device='cuda:7', grad_fn=<ExpBackward0>)\n",
      "tau2 before smooth: tensor([0.5423], device='cuda:7', grad_fn=<ExpBackward0>)\n",
      "tau2 before smooth: tensor([0.5570], device='cuda:7', grad_fn=<ExpBackward0>)\n",
      "[  20,   100] loss: 1.873, aux_loss: 0.000\n",
      "reg: 0.04465120658278465\n",
      "[  20,   200] loss: 1.844, aux_loss: 0.000\n",
      "reg: 0.02448306232690811\n",
      "[  20,   300] loss: 1.845, aux_loss: 0.000\n",
      "reg: 0.016737673431634903\n",
      "[  20,   400] loss: 1.844, aux_loss: 0.000\n",
      "reg: 0.012603123672306538\n",
      "tau2 before smooth: tensor([0.4944], device='cuda:7', grad_fn=<ExpBackward0>)\n",
      "tau2 before smooth: tensor([0.5449], device='cuda:7', grad_fn=<ExpBackward0>)\n",
      "tau2 before smooth: tensor([0.5565], device='cuda:7', grad_fn=<ExpBackward0>)\n",
      "[  21,   100] loss: 1.865, aux_loss: 0.000\n",
      "reg: 0.04415556788444519\n",
      "[  21,   200] loss: 1.837, aux_loss: 0.000\n",
      "reg: 0.024088051170110703\n",
      "[  21,   300] loss: 1.841, aux_loss: 0.000\n",
      "reg: 0.016460031270980835\n",
      "[  21,   400] loss: 1.826, aux_loss: 0.000\n",
      "reg: 0.012373468838632107\n",
      "tau2 before smooth: tensor([0.5008], device='cuda:7', grad_fn=<ExpBackward0>)\n",
      "tau2 before smooth: tensor([0.5493], device='cuda:7', grad_fn=<ExpBackward0>)\n",
      "tau2 before smooth: tensor([0.5584], device='cuda:7', grad_fn=<ExpBackward0>)\n",
      "[  22,   100] loss: 1.853, aux_loss: 0.000\n",
      "reg: 0.04133276641368866\n",
      "[  22,   200] loss: 1.820, aux_loss: 0.000\n",
      "reg: 0.022922851145267487\n",
      "[  22,   300] loss: 1.821, aux_loss: 0.000\n",
      "reg: 0.015835128724575043\n",
      "[  22,   400] loss: 1.816, aux_loss: 0.000\n",
      "reg: 0.01196770928800106\n",
      "tau2 before smooth: tensor([0.5220], device='cuda:7', grad_fn=<ExpBackward0>)\n",
      "tau2 before smooth: tensor([0.5518], device='cuda:7', grad_fn=<ExpBackward0>)\n",
      "tau2 before smooth: tensor([0.5560], device='cuda:7', grad_fn=<ExpBackward0>)\n",
      "[  23,   100] loss: 1.839, aux_loss: 0.000\n",
      "reg: 0.040836092084646225\n",
      "[  23,   200] loss: 1.810, aux_loss: 0.000\n",
      "reg: 0.022877350449562073\n",
      "[  23,   300] loss: 1.813, aux_loss: 0.000\n",
      "reg: 0.015727510675787926\n",
      "[  23,   400] loss: 1.807, aux_loss: 0.000\n",
      "reg: 0.011906130239367485\n",
      "tau2 before smooth: tensor([0.5254], device='cuda:7', grad_fn=<ExpBackward0>)\n",
      "tau2 before smooth: tensor([0.5515], device='cuda:7', grad_fn=<ExpBackward0>)\n",
      "tau2 before smooth: tensor([0.5573], device='cuda:7', grad_fn=<ExpBackward0>)\n",
      "[  24,   100] loss: 1.828, aux_loss: 0.000\n",
      "reg: 0.03926302120089531\n",
      "[  24,   200] loss: 1.812, aux_loss: 0.000\n",
      "reg: 0.02255147695541382\n",
      "[  24,   300] loss: 1.812, aux_loss: 0.000\n",
      "reg: 0.015667490661144257\n",
      "[  24,   400] loss: 1.813, aux_loss: 0.000\n",
      "reg: 0.011870835907757282\n",
      "tau2 before smooth: tensor([0.5269], device='cuda:7', grad_fn=<ExpBackward0>)\n",
      "tau2 before smooth: tensor([0.5502], device='cuda:7', grad_fn=<ExpBackward0>)\n",
      "tau2 before smooth: tensor([0.5577], device='cuda:7', grad_fn=<ExpBackward0>)\n",
      "[  25,   100] loss: 1.847, aux_loss: 0.000\n",
      "reg: 0.04424253851175308\n",
      "[  25,   200] loss: 1.812, aux_loss: 0.000\n",
      "reg: 0.023639310151338577\n",
      "[  25,   300] loss: 1.808, aux_loss: 0.000\n",
      "reg: 0.016119902953505516\n",
      "[  25,   400] loss: 1.808, aux_loss: 0.000\n",
      "reg: 0.012090491130948067\n",
      "tau2 before smooth: tensor([0.5179], device='cuda:7', grad_fn=<ExpBackward0>)\n",
      "tau2 before smooth: tensor([0.5479], device='cuda:7', grad_fn=<ExpBackward0>)\n",
      "tau2 before smooth: tensor([0.5586], device='cuda:7', grad_fn=<ExpBackward0>)\n",
      "[  26,   100] loss: 1.855, aux_loss: 0.000\n",
      "reg: 0.03854513540863991\n",
      "[  26,   200] loss: 1.811, aux_loss: 0.000\n",
      "reg: 0.022030383348464966\n",
      "[  26,   300] loss: 1.807, aux_loss: 0.000\n",
      "reg: 0.015414252877235413\n",
      "[  26,   400] loss: 1.805, aux_loss: 0.000\n",
      "reg: 0.011744188144803047\n",
      "tau2 before smooth: tensor([0.5336], device='cuda:7', grad_fn=<ExpBackward0>)\n",
      "tau2 before smooth: tensor([0.5493], device='cuda:7', grad_fn=<ExpBackward0>)\n",
      "tau2 before smooth: tensor([0.5582], device='cuda:7', grad_fn=<ExpBackward0>)\n",
      "[  27,   100] loss: 1.846, aux_loss: 0.000\n",
      "reg: 0.04088820144534111\n",
      "[  27,   200] loss: 1.809, aux_loss: 0.000\n",
      "reg: 0.022776436060667038\n",
      "[  27,   300] loss: 1.810, aux_loss: 0.000\n",
      "reg: 0.01572253182530403\n",
      "[  27,   400] loss: 1.805, aux_loss: 0.000\n",
      "reg: 0.011902546510100365\n",
      "tau2 before smooth: tensor([0.5249], device='cuda:7', grad_fn=<ExpBackward0>)\n",
      "tau2 before smooth: tensor([0.5488], device='cuda:7', grad_fn=<ExpBackward0>)\n",
      "tau2 before smooth: tensor([0.5603], device='cuda:7', grad_fn=<ExpBackward0>)\n",
      "[  28,   100] loss: 1.829, aux_loss: 0.000\n",
      "reg: 0.04328876733779907\n",
      "[  28,   200] loss: 1.803, aux_loss: 0.000\n",
      "reg: 0.023891808465123177\n",
      "[  28,   300] loss: 1.800, aux_loss: 0.000\n",
      "reg: 0.016397222876548767\n",
      "[  28,   400] loss: 1.802, aux_loss: 0.000\n",
      "reg: 0.012358255684375763\n",
      "tau2 before smooth: tensor([0.5151], device='cuda:7', grad_fn=<ExpBackward0>)\n",
      "tau2 before smooth: tensor([0.5314], device='cuda:7', grad_fn=<ExpBackward0>)\n",
      "tau2 before smooth: tensor([0.5594], device='cuda:7', grad_fn=<ExpBackward0>)\n",
      "[  29,   100] loss: 1.827, aux_loss: 0.000\n",
      "reg: 0.04520934820175171\n",
      "[  29,   200] loss: 1.800, aux_loss: 0.000\n",
      "reg: 0.025220530107617378\n",
      "[  29,   300] loss: 1.796, aux_loss: 0.000\n",
      "reg: 0.017223011702299118\n",
      "[  29,   400] loss: 1.802, aux_loss: 0.000\n",
      "reg: 0.012963994406163692\n",
      "tau2 before smooth: tensor([0.4789], device='cuda:7', grad_fn=<ExpBackward0>)\n",
      "tau2 before smooth: tensor([0.5376], device='cuda:7', grad_fn=<ExpBackward0>)\n",
      "tau2 before smooth: tensor([0.5601], device='cuda:7', grad_fn=<ExpBackward0>)\n",
      "[  30,   100] loss: 1.864, aux_loss: 0.000\n",
      "reg: 0.34308964014053345\n",
      "[  30,   200] loss: 1.840, aux_loss: 0.000\n",
      "reg: 0.24457523226737976\n",
      "[  30,   300] loss: 1.834, aux_loss: 0.000\n",
      "reg: 0.1886116862297058\n",
      "[  30,   400] loss: 1.814, aux_loss: 0.000\n",
      "reg: 0.1524261236190796\n",
      "tau2 before smooth: tensor([0.1463], device='cuda:7', grad_fn=<ExpBackward0>)\n",
      "tau2 before smooth: tensor([0.1490], device='cuda:7', grad_fn=<ExpBackward0>)\n",
      "tau2 before smooth: tensor([0.1491], device='cuda:7', grad_fn=<ExpBackward0>)\n",
      "[  31,   100] loss: 1.844, aux_loss: 0.000\n",
      "reg: 0.4082306921482086\n",
      "[  31,   200] loss: 1.834, aux_loss: 0.000\n",
      "reg: 0.3065195083618164\n",
      "[  31,   300] loss: 1.835, aux_loss: 0.000\n",
      "reg: 0.24578428268432617\n",
      "[  31,   400] loss: 1.841, aux_loss: 0.000\n",
      "reg: 0.20248576998710632\n",
      "tau2 before smooth: tensor([0.1212], device='cuda:7', grad_fn=<ExpBackward0>)\n",
      "tau2 before smooth: tensor([0.1301], device='cuda:7', grad_fn=<ExpBackward0>)\n",
      "tau2 before smooth: tensor([0.1317], device='cuda:7', grad_fn=<ExpBackward0>)\n",
      "[  32,   100] loss: 1.834, aux_loss: 0.000\n",
      "reg: 0.44721686840057373\n",
      "[  32,   200] loss: 1.832, aux_loss: 0.000\n",
      "reg: 0.35130003094673157\n",
      "[  32,   300] loss: 1.826, aux_loss: 0.000\n",
      "reg: 0.28933972120285034\n",
      "[  32,   400] loss: 1.826, aux_loss: 0.000\n",
      "reg: 0.2454856038093567\n",
      "tau2 before smooth: tensor([0.1068], device='cuda:7', grad_fn=<ExpBackward0>)\n",
      "tau2 before smooth: tensor([0.1210], device='cuda:7', grad_fn=<ExpBackward0>)\n",
      "tau2 before smooth: tensor([0.1232], device='cuda:7', grad_fn=<ExpBackward0>)\n",
      "[  33,   100] loss: 1.823, aux_loss: 0.000\n",
      "reg: 0.45827406644821167\n",
      "[  33,   200] loss: 1.818, aux_loss: 0.000\n",
      "reg: 0.36406198143959045\n",
      "[  33,   300] loss: 1.827, aux_loss: 0.000\n",
      "reg: 0.3021071255207062\n",
      "[  33,   400] loss: 1.830, aux_loss: 0.000\n",
      "reg: 0.2561670243740082\n",
      "tau2 before smooth: tensor([0.1082], device='cuda:7', grad_fn=<ExpBackward0>)\n",
      "tau2 before smooth: tensor([0.1183], device='cuda:7', grad_fn=<ExpBackward0>)\n",
      "tau2 before smooth: tensor([0.1183], device='cuda:7', grad_fn=<ExpBackward0>)\n",
      "[  34,   100] loss: 1.828, aux_loss: 0.000\n",
      "reg: 0.4681212902069092\n",
      "[  34,   200] loss: 1.856, aux_loss: 0.000\n",
      "reg: 0.38005608320236206\n",
      "[  34,   300] loss: 1.851, aux_loss: 0.000\n",
      "reg: 0.32008445262908936\n",
      "[  34,   400] loss: 1.853, aux_loss: 0.000\n",
      "reg: 0.27474245429039\n",
      "tau2 before smooth: tensor([0.1016], device='cuda:7', grad_fn=<ExpBackward0>)\n",
      "tau2 before smooth: tensor([0.1139], device='cuda:7', grad_fn=<ExpBackward0>)\n",
      "tau2 before smooth: tensor([0.1157], device='cuda:7', grad_fn=<ExpBackward0>)\n",
      "[  35,   100] loss: 1.847, aux_loss: 0.000\n",
      "reg: 0.47455620765686035\n",
      "[  35,   200] loss: 1.843, aux_loss: 0.000\n",
      "reg: 0.3851606249809265\n",
      "[  35,   300] loss: 1.844, aux_loss: 0.000\n",
      "reg: 0.32350170612335205\n",
      "[  35,   400] loss: 1.847, aux_loss: 0.000\n",
      "reg: 0.276689350605011\n",
      "tau2 before smooth: tensor([0.1035], device='cuda:7', grad_fn=<ExpBackward0>)\n",
      "tau2 before smooth: tensor([0.1123], device='cuda:7', grad_fn=<ExpBackward0>)\n",
      "tau2 before smooth: tensor([0.1140], device='cuda:7', grad_fn=<ExpBackward0>)\n",
      "[  36,   100] loss: 1.823, aux_loss: 0.000\n",
      "reg: 0.4742875099182129\n",
      "[  36,   200] loss: 1.817, aux_loss: 0.000\n",
      "reg: 0.38503575325012207\n",
      "[  36,   300] loss: 1.819, aux_loss: 0.000\n",
      "reg: 0.3234789967536926\n",
      "[  36,   400] loss: 1.813, aux_loss: 0.000\n",
      "reg: 0.278056263923645\n",
      "tau2 before smooth: tensor([0.1029], device='cuda:7', grad_fn=<ExpBackward0>)\n",
      "tau2 before smooth: tensor([0.1117], device='cuda:7', grad_fn=<ExpBackward0>)\n",
      "tau2 before smooth: tensor([0.1128], device='cuda:7', grad_fn=<ExpBackward0>)\n",
      "[  37,   100] loss: 1.820, aux_loss: 0.000\n",
      "reg: 0.477924644947052\n",
      "[  37,   200] loss: 1.841, aux_loss: 0.000\n",
      "reg: 0.39718323945999146\n",
      "[  37,   300] loss: 1.844, aux_loss: 0.000\n",
      "reg: 0.3371373414993286\n",
      "[  37,   400] loss: 1.833, aux_loss: 0.000\n",
      "reg: 0.2920626997947693\n",
      "tau2 before smooth: tensor([0.0972], device='cuda:7', grad_fn=<ExpBackward0>)\n",
      "tau2 before smooth: tensor([0.1112], device='cuda:7', grad_fn=<ExpBackward0>)\n",
      "tau2 before smooth: tensor([0.1119], device='cuda:7', grad_fn=<ExpBackward0>)\n",
      "[  38,   100] loss: 1.843, aux_loss: 0.000\n",
      "reg: 0.4836646616458893\n",
      "[  38,   200] loss: 1.843, aux_loss: 0.000\n",
      "reg: 0.39644524455070496\n",
      "[  38,   300] loss: 1.838, aux_loss: 0.000\n",
      "reg: 0.3355395197868347\n",
      "[  38,   400] loss: 1.843, aux_loss: 0.000\n",
      "reg: 0.28991007804870605\n",
      "tau2 before smooth: tensor([0.1003], device='cuda:7', grad_fn=<ExpBackward0>)\n",
      "tau2 before smooth: tensor([0.1099], device='cuda:7', grad_fn=<ExpBackward0>)\n",
      "tau2 before smooth: tensor([0.1113], device='cuda:7', grad_fn=<ExpBackward0>)\n",
      "[  39,   100] loss: 1.838, aux_loss: 0.000\n",
      "reg: 0.4879966378211975\n",
      "[  39,   200] loss: 1.842, aux_loss: 0.000\n",
      "reg: 0.40371665358543396\n",
      "[  39,   300] loss: 1.840, aux_loss: 0.000\n",
      "reg: 0.3439285159111023\n",
      "[  39,   400] loss: 1.833, aux_loss: 0.000\n",
      "reg: 0.298240065574646\n",
      "tau2 before smooth: tensor([0.0955], device='cuda:7', grad_fn=<ExpBackward0>)\n",
      "tau2 before smooth: tensor([0.1105], device='cuda:7', grad_fn=<ExpBackward0>)\n",
      "tau2 before smooth: tensor([0.1110], device='cuda:7', grad_fn=<ExpBackward0>)\n",
      "[  40,   100] loss: 1.824, aux_loss: 0.000\n",
      "reg: 0.4852152466773987\n",
      "[  40,   200] loss: 1.837, aux_loss: 0.000\n",
      "reg: 0.39920902252197266\n",
      "[  40,   300] loss: 1.839, aux_loss: 0.000\n",
      "reg: 0.3388247787952423\n",
      "[  40,   400] loss: 1.834, aux_loss: 0.000\n",
      "reg: 0.2931366562843323\n",
      "tau2 before smooth: tensor([0.0983], device='cuda:7', grad_fn=<ExpBackward0>)\n",
      "tau2 before smooth: tensor([0.1101], device='cuda:7', grad_fn=<ExpBackward0>)\n",
      "tau2 before smooth: tensor([0.1107], device='cuda:7', grad_fn=<ExpBackward0>)\n",
      "[  41,   100] loss: 1.837, aux_loss: 0.000\n",
      "reg: 0.4827461838722229\n",
      "[  41,   200] loss: 1.836, aux_loss: 0.000\n",
      "reg: 0.39745044708251953\n",
      "[  41,   300] loss: 1.839, aux_loss: 0.000\n",
      "reg: 0.3361177444458008\n",
      "[  41,   400] loss: 1.836, aux_loss: 0.000\n",
      "reg: 0.2904776632785797\n",
      "tau2 before smooth: tensor([0.1005], device='cuda:7', grad_fn=<ExpBackward0>)\n",
      "tau2 before smooth: tensor([0.1091], device='cuda:7', grad_fn=<ExpBackward0>)\n",
      "tau2 before smooth: tensor([0.1105], device='cuda:7', grad_fn=<ExpBackward0>)\n",
      "[  42,   100] loss: 1.839, aux_loss: 0.000\n",
      "reg: 0.4806491434574127\n",
      "[  42,   200] loss: 1.840, aux_loss: 0.000\n",
      "reg: 0.39238864183425903\n",
      "[  42,   300] loss: 1.831, aux_loss: 0.000\n",
      "reg: 0.33150380849838257\n",
      "[  42,   400] loss: 1.835, aux_loss: 0.000\n",
      "reg: 0.28561025857925415\n",
      "tau2 before smooth: tensor([0.1028], device='cuda:7', grad_fn=<ExpBackward0>)\n",
      "tau2 before smooth: tensor([0.1095], device='cuda:7', grad_fn=<ExpBackward0>)\n",
      "tau2 before smooth: tensor([0.1104], device='cuda:7', grad_fn=<ExpBackward0>)\n",
      "[  43,   100] loss: 1.836, aux_loss: 0.000\n",
      "reg: 0.479155957698822\n",
      "[  43,   200] loss: 1.833, aux_loss: 0.000\n",
      "reg: 0.3901226818561554\n",
      "[  43,   300] loss: 1.832, aux_loss: 0.000\n",
      "reg: 0.3286522626876831\n",
      "[  43,   400] loss: 1.834, aux_loss: 0.000\n",
      "reg: 0.2829231917858124\n",
      "tau2 before smooth: tensor([0.1051], device='cuda:7', grad_fn=<ExpBackward0>)\n",
      "tau2 before smooth: tensor([0.1098], device='cuda:7', grad_fn=<ExpBackward0>)\n",
      "tau2 before smooth: tensor([0.1103], device='cuda:7', grad_fn=<ExpBackward0>)\n",
      "[  44,   100] loss: 1.833, aux_loss: 0.000\n",
      "reg: 0.4775230884552002\n",
      "[  44,   200] loss: 1.832, aux_loss: 0.000\n",
      "reg: 0.3883802890777588\n",
      "[  44,   300] loss: 1.829, aux_loss: 0.000\n",
      "reg: 0.3269352316856384\n",
      "[  44,   400] loss: 1.828, aux_loss: 0.000\n",
      "reg: 0.2810288369655609\n",
      "tau2 before smooth: tensor([0.1055], device='cuda:7', grad_fn=<ExpBackward0>)\n",
      "tau2 before smooth: tensor([0.1096], device='cuda:7', grad_fn=<ExpBackward0>)\n",
      "tau2 before smooth: tensor([0.1102], device='cuda:7', grad_fn=<ExpBackward0>)\n",
      "[  45,   100] loss: 1.828, aux_loss: 0.000\n",
      "reg: 0.5956974029541016\n",
      "[  45,   200] loss: 1.832, aux_loss: 0.000\n",
      "reg: 0.5797793865203857\n",
      "[  45,   300] loss: 1.830, aux_loss: 0.000\n",
      "reg: 0.565251350402832\n",
      "[  45,   400] loss: 1.830, aux_loss: 0.000\n",
      "reg: 0.5517871975898743\n",
      "tau2 before smooth: tensor([0.0740], device='cuda:7', grad_fn=<ExpBackward0>)\n",
      "tau2 before smooth: tensor([0.0745], device='cuda:7', grad_fn=<ExpBackward0>)\n",
      "tau2 before smooth: tensor([0.0745], device='cuda:7', grad_fn=<ExpBackward0>)\n",
      "[  46,   100] loss: 1.826, aux_loss: 0.000\n",
      "reg: 0.5974799394607544\n",
      "[  46,   200] loss: 1.832, aux_loss: 0.000\n",
      "reg: 0.5834386348724365\n",
      "[  46,   300] loss: 1.830, aux_loss: 0.000\n",
      "reg: 0.570284366607666\n",
      "[  46,   400] loss: 1.833, aux_loss: 0.000\n",
      "reg: 0.5578645467758179\n",
      "tau2 before smooth: tensor([0.0737], device='cuda:7', grad_fn=<ExpBackward0>)\n",
      "tau2 before smooth: tensor([0.0739], device='cuda:7', grad_fn=<ExpBackward0>)\n",
      "tau2 before smooth: tensor([0.0740], device='cuda:7', grad_fn=<ExpBackward0>)\n",
      "[  47,   100] loss: 1.832, aux_loss: 0.000\n",
      "reg: 0.5984926819801331\n",
      "[  47,   200] loss: 1.832, aux_loss: 0.000\n",
      "reg: 0.5852403044700623\n",
      "[  47,   300] loss: 1.832, aux_loss: 0.000\n",
      "reg: 0.5727288722991943\n",
      "[  47,   400] loss: 1.827, aux_loss: 0.000\n",
      "reg: 0.5608198046684265\n",
      "tau2 before smooth: tensor([0.0736], device='cuda:7', grad_fn=<ExpBackward0>)\n",
      "tau2 before smooth: tensor([0.0737], device='cuda:7', grad_fn=<ExpBackward0>)\n",
      "tau2 before smooth: tensor([0.0737], device='cuda:7', grad_fn=<ExpBackward0>)\n",
      "[  48,   100] loss: 1.830, aux_loss: 0.000\n",
      "reg: 0.5990365147590637\n",
      "[  48,   200] loss: 1.829, aux_loss: 0.000\n",
      "reg: 0.586272120475769\n",
      "[  48,   300] loss: 1.827, aux_loss: 0.000\n",
      "reg: 0.5741127729415894\n",
      "[  48,   400] loss: 1.831, aux_loss: 0.000\n",
      "reg: 0.5624709129333496\n",
      "tau2 before smooth: tensor([0.0735], device='cuda:7', grad_fn=<ExpBackward0>)\n",
      "tau2 before smooth: tensor([0.0736], device='cuda:7', grad_fn=<ExpBackward0>)\n",
      "tau2 before smooth: tensor([0.0736], device='cuda:7', grad_fn=<ExpBackward0>)\n",
      "[  49,   100] loss: 1.832, aux_loss: 0.000\n",
      "reg: 0.5993508100509644\n",
      "[  49,   200] loss: 1.830, aux_loss: 0.000\n",
      "reg: 0.5868487358093262\n",
      "[  49,   300] loss: 1.826, aux_loss: 0.000\n",
      "reg: 0.5748937129974365\n",
      "[  49,   400] loss: 1.832, aux_loss: 0.000\n",
      "reg: 0.5634366869926453\n",
      "tau2 before smooth: tensor([0.0734], device='cuda:7', grad_fn=<ExpBackward0>)\n",
      "tau2 before smooth: tensor([0.0735], device='cuda:7', grad_fn=<ExpBackward0>)\n",
      "tau2 before smooth: tensor([0.0735], device='cuda:7', grad_fn=<ExpBackward0>)\n",
      "[  50,   100] loss: 1.831, aux_loss: 0.000\n",
      "reg: 0.5995399951934814\n",
      "[  50,   200] loss: 1.833, aux_loss: 0.000\n",
      "reg: 0.5871992707252502\n",
      "[  50,   300] loss: 1.830, aux_loss: 0.000\n",
      "reg: 0.5753768682479858\n",
      "[  50,   400] loss: 1.828, aux_loss: 0.000\n",
      "reg: 0.563987672328949\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "# 假设 LeNet5 类已定义 (使用上一个回答中的代码)\n",
    "\n",
    "# --- 0. 环境准备：设备、数据加载 ---\n",
    "\n",
    "# 检查是否有GPU可用\n",
    "\n",
    "# net = MoE_vision(args.input_size, args.num_experts, args.hidden_size, args.depth, args.output_size, args.k, args.loss_coef,activation=nn.ReLU()).to(device)\n",
    "\n",
    "# 数据预处理（与之前相同）\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "# 重新加载训练集和测试集 (下载/加载)\n",
    "train_dataset = torchvision.datasets.MNIST(root='/home/zhy/Zhou/mixture_of_experts/_image_run/MINST/', train=True, transform=transform, download=True)\n",
    "test_dataset = torchvision.datasets.MNIST(root='/home/zhy/Zhou/mixture_of_experts/_image_run/MINST/', train=False, transform=transform, download=True)\n",
    "\n",
    "# 定义超参数\n",
    "BATCH_SIZE = args.batch_size # LeNet通常可以使用较大的Batch Size\n",
    "epochs = args.epochs # 沿用您最初设定的epochs\n",
    "\n",
    "# 数据加载器\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "\n",
    "# --- 1. 模型、优化器和损失函数设置 ---\n",
    "\n",
    "# 实例化模型并移动到指定设备\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = optim.Adam(net.parameters(), lr=1e-2)\n",
    "scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[30, 45], gamma=0.1)\n",
    "\n",
    "\n",
    "# --- 2. 训练主循环 ---\n",
    "print(\"\\n--- 开始训练 ---\")\n",
    "\n",
    "\n",
    "step_count=args.smooth_steps\n",
    "\n",
    "for epoch in range(args.epochs):  # loop over the dataset multiple times\n",
    "    net.train()\n",
    "    running_loss = 0.0\n",
    "    running_aux_loss = 0.0\n",
    "    scheduler.step()\n",
    "    \n",
    "    step_count -=1\n",
    "    tempstep=step_count\n",
    "    for j in range(len(net.moe)):\n",
    "        print(\"tau2 before smooth:\",net.moe[j].tau2)\n",
    "        if net.moe[j].smooth and tempstep<=0:\n",
    "            with torch.no_grad(): #0.08\n",
    "                net.moe[j].alpha2.copy_(torch.log(torch.tensor([0.07], device=device)))\n",
    "            net.moe[j].smoothing(epoch,args.smooth_lb)\n",
    "            step_count=args.smooth_steps\n",
    "        elif tempstep<=0 :\n",
    "            with torch.no_grad(): #0.08\n",
    "                net.moe[j].alpha2.copy_(torch.log(torch.tensor([0.07], device=device)))\n",
    "            \n",
    "            net.moe[j].smoothing(epoch,args.smooth_lb)\n",
    "            step_count=args.smooth_steps/args.smooth_steps\n",
    "        \n",
    "        \n",
    "\n",
    "    \n",
    "\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "\n",
    "\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        # inputs, labels = data\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "        \n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "\n",
    "        # outputs, aux_loss= MoE(inputs)\n",
    "        outputs ,auxloss= net(inputs)\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "        reg=0\n",
    "        for j in range(len(net.moe)):\n",
    "            lambda_l2 = 1e-3\n",
    "            # \n",
    "            reg += lambda_l2 * (1/(net.moe[j].tau2) **2).sum()\n",
    "        total_loss =loss+ reg\n",
    "\n",
    "        \n",
    "        # total_loss =loss+ aux_loss\n",
    "        \n",
    "\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        # running_aux_loss += aux_loss.item()\n",
    "        \n",
    "        if i % 100== 99:    # print every 100 mini-batches\n",
    "            print(f'[{epoch + 1:4d}, {i + 1:5d}] loss: {running_loss / 100:.3f}, aux_loss: {running_aux_loss/100:.3f}')\n",
    "            print(\"reg:\",reg.item())\n",
    "            running_loss = 0.0\n",
    "            running_aux_loss = 0.0\n",
    "\n",
    "\n",
    "print('Finished Training')\n",
    "\n",
    "\n",
    "# 保存训练好的模型（可选）\n",
    "PATH = '/home/zhy/Zhou/mixture_of_experts/_image_run/saved_cnn/mnist_lenet5.pth'\n",
    "torch.save(net.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "41081992",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Trainable parameters ===\n",
      "Total trainable params: 1713\n",
      "\n",
      "\n",
      "在 10000 张测试图片上的准确率: 23.03 %\n"
     ]
    }
   ],
   "source": [
    "net =  MoE_vision(args.input_size, args.num_experts, args.hidden_size, args.depth, args.output_size, args.k, args.loss_coef,activation=nn.ReLU()).to(device)\n",
    "\n",
    "net.load_state_dict(torch.load(PATH, weights_only=True))\n",
    "# 运行评估函数（可选，但推荐）\n",
    "def evaluate_model():\n",
    "    net.eval() # 设置模型为评估模式\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad(): # 评估时不需要计算梯度\n",
    "        for data in test_loader:\n",
    "            images, labels = data[0].to(device), data[1].to(device)\n",
    "            outputs,loss = net(images)\n",
    "            _, predicted = torch.max(outputs.data, 1) # 获取预测结果\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print(f'\\n在 10000 张测试图片上的准确率: {100 * correct / total:.2f} %')\n",
    "\n",
    "evaluate_model()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca5c748",
   "metadata": {},
   "source": [
    "93.57 %\n",
    "70.66 放在内部 3 expert\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8553be9",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-zqh",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

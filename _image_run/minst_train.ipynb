{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263246a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt # 用于可视化，可选\n",
    "\n",
    "# 1. 定义数据预处理步骤\n",
    "# 转换为 Tensor，并进行标准化（这是深度学习的标准步骤）\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    # MNIST的均值和标准差，用于将像素值归一化到 [-1, 1] 左右\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "# 2. 下载并加载数据集\n",
    "# root='.' 表示下载到当前文件夹\n",
    "# train=True 表示下载训练集 (共60000张)\n",
    "# download=True 表示如果本地没有，则下载\n",
    "print(\"正在下载/加载 MNIST 训练数据集...\")\n",
    "train_dataset = torchvision.datasets.MNIST(root='/home/zhy/Zhou/mixture_of_experts/_image_run/MINST/', \n",
    "                                           train=True, \n",
    "                                           transform=transform, \n",
    "                                           download=True)\n",
    "print(\"数据集加载完成。\")\n",
    "\n",
    "# 3. 使用 DataLoader 载入数据\n",
    "# 设置一个较小的 batch_size 方便查看结果\n",
    "batch_size = 4\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=True)\n",
    "\n",
    "# 4. 取出并打印第一批数据\n",
    "# 迭代器next()方法取出第一批数据\n",
    "data_iter = iter(train_loader)\n",
    "images, labels = next(data_iter)\n",
    "\n",
    "print(\"\\n--- 第一批数据结果 ---\")\n",
    "print(f\"图片张量形状 (Batch, C, H, W): {images.shape}\")\n",
    "print(f\"标签张量形状 (Batch): {labels.shape}\\n\")\n",
    "\n",
    "# 5. 打印前4张图片的信息\n",
    "for i in range(batch_size):\n",
    "    print(f\"--- 图像 {i+1} ---\")\n",
    "    \n",
    "    # 打印标签\n",
    "    print(f\"  标签: {labels[i].item()}\")\n",
    "    \n",
    "    # 打印部分像素值 (张量切片)\n",
    "    # 形状是 (1, 28, 28)，这里打印第0通道的前5行5列的像素值\n",
    "    print(\"  部分像素值 (前5x5):\")\n",
    "    print(images[i, 0, :5, :5])\n",
    "    print(\"-\" * 20)\n",
    "# （接上面的代码，在第5步之后运行）\n",
    "fig, axes = plt.subplots(1, batch_size, figsize=(10, 3))\n",
    "for i in range(batch_size):\n",
    "    # 移除标准化并转为Numpy数组\n",
    "    img = images[i].numpy().transpose((1, 2, 0)) # 将(C, H, W)转为(H, W, C)\n",
    "    \n",
    "    # 因为图像是归一化过的，这里需要简单的反归一化来正确显示\n",
    "    # 仅为了显示效果，我们直接裁剪到有效范围\n",
    "    axes[i].imshow(img.squeeze(), cmap='gray') \n",
    "    axes[i].set_title(f\"Label: {labels[i].item()}\")\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d60c569",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    \"\"\"\n",
    "    经典的 LeNet-5 架构，针对 28x28 灰度图 (MNIST) 进行了调整。\n",
    "    \n",
    "    原始 LeNet-5 的输入是 32x32，这里使用 28x28，\n",
    "    但整体的 Conv -> Pool -> Conv -> Pool -> FC 结构保持不变。\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        # 1. 卷积层 C1\n",
    "        # 输入: (1, 28, 28)\n",
    "        # 输出: (6, 28, 28)  (6个 5x5 卷积核, padding=2 保持尺寸)\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5, padding=2)\n",
    "        # 激活函数: 使用 ReLU 取代 sigmoid/tanh，以加速训练\n",
    "        \n",
    "        # 2. 池化层 S2 (平均池化)\n",
    "        # 输入: (6, 28, 28)\n",
    "        # 输出: (6, 14, 14)  (2x2 池化，步长为 2)\n",
    "        self.pool1 = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # 3. 卷积层 C3\n",
    "        # 输入: (6, 14, 14)\n",
    "        # 输出: (16, 10, 10) (16个 5x5 卷积核, 无 padding)\n",
    "        self.conv2 = nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5)\n",
    "        \n",
    "        # 4. 池化层 S4 (平均池化)\n",
    "        # 输入: (16, 10, 10)\n",
    "        # 输出: (16, 5, 5)   (2x2 池化，步长为 2)\n",
    "        self.pool2 = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # 计算进入全连接层时的展平尺寸: 16 * 5 * 5 = 400\n",
    "        \n",
    "        # 5. 全连接层 F5\n",
    "        # 输入: 16 * 5 * 5 = 400\n",
    "        # 输出: 120\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        \n",
    "        # 6. 全连接层 F6\n",
    "        # 输入: 120\n",
    "        # 输出: 84\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        \n",
    "        # 7. 输出层 Output\n",
    "        # 输入: 84\n",
    "        # 输出: num_classes (10)\n",
    "        self.fc3 = nn.Linear(84, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # C1: Conv -> ReLU -> Pool\n",
    "        x = self.pool1(F.relu(self.conv1(x)))\n",
    "        \n",
    "        # C3: Conv -> ReLU -> Pool\n",
    "        x = self.pool2(F.relu(self.conv2(x)))\n",
    "        \n",
    "        # 展平操作 (Flatten): (Batch, 16, 5, 5) -> (Batch, 400)\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        \n",
    "        # F5: FC -> ReLU\n",
    "        x = F.relu(self.fc1(x))\n",
    "        \n",
    "        # F6: FC -> ReLU\n",
    "        x = F.relu(self.fc2(x))\n",
    "        \n",
    "        # Output: FC\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        # 注意: 训练时通常直接输出 logits，不在这里加 softmax\n",
    "        return x\n",
    "\n",
    "# 实例化 LeNet-5 模型\n",
    "net = Net(num_classes=10)\n",
    "\n",
    "# 打印模型结构\n",
    "print(\"--- LeNet-5 模型结构 ---\")\n",
    "print(net)\n",
    "\n",
    "# 检查输入/输出尺寸\n",
    "# 假设批量大小为 64，输入是 (64, 1, 28, 28)\n",
    "input_tensor = torch.randn(64, 1, 28, 28) \n",
    "output = net(input_tensor)\n",
    "print(f\"\\n输入尺寸: {input_tensor.shape}\")\n",
    "print(f\"输出尺寸: {output.shape}\") \n",
    "# 预期输出: torch.Size([64, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce8288c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "# 假设 LeNet5 类已定义 (使用上一个回答中的代码)\n",
    "\n",
    "# --- 0. 环境准备：设备、数据加载 ---\n",
    "\n",
    "# 检查是否有GPU可用\n",
    "device = torch.device(\"cuda:7\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"使用的设备: {device}\")\n",
    "\n",
    "# 数据预处理（与之前相同）\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "# 重新加载训练集和测试集 (下载/加载)\n",
    "train_dataset = torchvision.datasets.MNIST(root='/home/zhy/Zhou/mixture_of_experts/_image_run/MINST/', train=True, transform=transform, download=True)\n",
    "test_dataset = torchvision.datasets.MNIST(root='/home/zhy/Zhou/mixture_of_experts/_image_run/MINST/', train=False, transform=transform, download=True)\n",
    "\n",
    "# 定义超参数\n",
    "BATCH_SIZE = 64 # LeNet通常可以使用较大的Batch Size\n",
    "epochs = 60 # 沿用您最初设定的epochs\n",
    "\n",
    "# 数据加载器\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "\n",
    "# --- 1. 模型、优化器和损失函数设置 ---\n",
    "\n",
    "# 实例化模型并移动到指定设备\n",
    "net = Net(num_classes=10).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
    "scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[30, 45], gamma=0.1)\n",
    "\n",
    "\n",
    "# --- 2. 训练主循环 ---\n",
    "print(\"\\n--- 开始训练 ---\")\n",
    "\n",
    "for epoch in range(epochs):  # 循环训练 epochs 次\n",
    "    net.train() # 设置模型为训练模式\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    # 遍历训练数据加载器中的所有批次\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        # 获取输入数据和标签，并移动到设备上\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        # 每隔 190 个 mini-batches 打印一次 Loss\n",
    "        # 您的数据集总共 60000/64 = 937.5 个 batch，所以 190 是一个很好的间隔\n",
    "        if i % 190 == 189: # 这里的 189 确保是第 190, 380, ... 个 batch 结束时打印\n",
    "            print(f'[{epoch + 1:2d}, {i + 1:5d}] loss: {running_loss / 190:.3f}')\n",
    "            running_loss = 0.0 # 清零累加器，开始统计下一个 190 batch 的平均 Loss\n",
    "\n",
    "    scheduler.step()\n",
    "    current_lr = scheduler.get_last_lr()[0]\n",
    "    print(f\"Epoch {epoch + 1} 结束, 当前学习率 (LR): {current_lr:.6f}\")\n",
    "\n",
    "print('训练结束!')\n",
    "\n",
    "\n",
    "\n",
    "# 保存训练好的模型（可选）\n",
    "PATH = '/home/zhy/Zhou/mixture_of_experts/_image_run/saved_cnn/mnist_lenet5.pth'\n",
    "torch.save(net.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccae6a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net(num_classes=10).to(device)\n",
    "net.load_state_dict(torch.load(PATH, weights_only=True))\n",
    "# 运行评估函数（可选，但推荐）\n",
    "def evaluate_model():\n",
    "    net.eval() # 设置模型为评估模式\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad(): # 评估时不需要计算梯度\n",
    "        for data in test_loader:\n",
    "            images, labels = data[0].to(device), data[1].to(device)\n",
    "            outputs = net(images)\n",
    "            _, predicted = torch.max(outputs.data, 1) # 获取预测结果\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print(f'\\n在 10000 张测试图片上的准确率: {100 * correct / total:.2f} %')\n",
    "\n",
    "evaluate_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "moe_zhy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

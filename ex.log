(env-zqh) /home/zhy/anaconda3/envs/env-zqh/bin/python /home/zhy/Zhou/mixture-of-experts/main.py
data_x shape: torch.Size([250, 1]),data_y shape: torch.Size([250, 1]) 
[2025-06-19 03:38:26] INFO: 【开始】train_loop
Step 1/10000 - loss: 2.50499680 -aux_loss: 0.00018171-rank: 8,[8, 8, 8]
Step 101/10000 - loss: 0.52712361 -aux_loss: 0.00039411-rank: 9,[10, 12, 14]
Step 201/10000 - loss: 0.50421986 -aux_loss: 0.00007515-rank: 10,[12, 14, 16]
Step 301/10000 - loss: 0.48756582 -aux_loss: 0.00010586-rank: 10,[12, 15, 18]
Step 401/10000 - loss: 0.49718860 -aux_loss: 0.00022433-rank: 10,[11, 14, 17]
Step 501/10000 - loss: 0.48893016 -aux_loss: 0.00005990-rank: 10,[12, 16, 18]
Step 601/10000 - loss: 0.48831555 -aux_loss: 0.00006422-rank: 10,[12, 16, 18]
Step 701/10000 - loss: 0.47979114 -aux_loss: 0.00012631-rank: 10,[12, 17, 21]
Step 801/10000 - loss: 0.45588136 -aux_loss: 0.00003749-rank: 10,[13, 18, 24]
Step 901/10000 - loss: 0.44320139 -aux_loss: 0.00019345-rank: 10,[14, 20, 25]
Step 1001/10000 - loss: 0.43589487 -aux_loss: 0.00020184-rank: 10,[15, 21, 27]
Step 1101/10000 - loss: 0.42701919 -aux_loss: 0.00023341-rank: 9,[13, 21, 28]
Step 1201/10000 - loss: 0.42425401 -aux_loss: 0.00010542-rank: 9,[13, 23, 29]
Step 1301/10000 - loss: 0.40183893 -aux_loss: 0.00004620-rank: 9,[14, 24, 31]
Step 1401/10000 - loss: 0.37522544 -aux_loss: 0.00020684-rank: 10,[16, 25, 35]
Step 1501/10000 - loss: 0.35708174 -aux_loss: 0.00013771-rank: 9,[16, 27, 36]
Step 1601/10000 - loss: 0.34046039 -aux_loss: 0.00008416-rank: 9,[17, 29, 37]
Step 1701/10000 - loss: 0.32722576 -aux_loss: 0.00004721-rank: 10,[18, 31, 38]
Step 1801/10000 - loss: 0.30438432 -aux_loss: 0.00012959-rank: 10,[20, 31, 39]
Step 1901/10000 - loss: 0.26595945 -aux_loss: 0.00017285-rank: 10,[22, 32, 41]
Step 2001/10000 - loss: 0.22693013 -aux_loss: 0.00010246-rank: 10,[23, 32, 43]
Step 2101/10000 - loss: 0.21384614 -aux_loss: 0.00008817-rank: 10,[23, 33, 42]
Step 2201/10000 - loss: 0.17444102 -aux_loss: 0.00011400-rank: 10,[25, 33, 44]
Step 2301/10000 - loss: 0.09660141 -aux_loss: 0.00018045-rank: 10,[27, 34, 47]
Step 2401/10000 - loss: 0.08183826 -aux_loss: 0.00026488-rank: 10,[28, 34, 47]
Step 2501/10000 - loss: 0.07746657 -aux_loss: 0.00006492-rank: 10,[27, 34, 46]
Step 2601/10000 - loss: 0.07500568 -aux_loss: 0.00012481-rank: 10,[26, 34, 46]
Step 2701/10000 - loss: 0.05890658 -aux_loss: 0.00031802-rank: 10,[28, 34, 47]
Step 2801/10000 - loss: 0.01917428 -aux_loss: 0.00030412-rank: 10,[29, 34, 48]
Step 2901/10000 - loss: 0.00572582 -aux_loss: 0.00022719-rank: 10,[30, 34, 47]
Step 3001/10000 - loss: 0.00314525 -aux_loss: 0.00021606-rank: 10,[30, 34, 47]
Step 3101/10000 - loss: 0.00349104 -aux_loss: 0.00010419-rank: 10,[29, 34, 47]
Step 3201/10000 - loss: 0.00253259 -aux_loss: 0.00022664-rank: 10,[29, 35, 47]
Step 3301/10000 - loss: 0.00360250 -aux_loss: 0.00001391-rank: 10,[29, 35, 47]
Step 3401/10000 - loss: 0.00243096 -aux_loss: 0.00002219-rank: 10,[28, 35, 47]
Step 3501/10000 - loss: 0.00181421 -aux_loss: 0.00002920-rank: 10,[28, 35, 47]
Step 3601/10000 - loss: 0.00292440 -aux_loss: 0.00007535-rank: 10,[28, 34, 47]
Step 3701/10000 - loss: 0.00235090 -aux_loss: 0.00007208-rank: 10,[28, 35, 47]
Step 3801/10000 - loss: 0.00182528 -aux_loss: 0.00002178-rank: 10,[28, 36, 46]
Step 3901/10000 - loss: 0.00311215 -aux_loss: 0.00003971-rank: 10,[28, 36, 46]
Step 4001/10000 - loss: 0.00319370 -aux_loss: 0.00001041-rank: 10,[28, 36, 46]
Step 4101/10000 - loss: 0.00137548 -aux_loss: 0.00008584-rank: 10,[28, 36, 46]
Step 4201/10000 - loss: 0.00183146 -aux_loss: 0.00003230-rank: 10,[28, 36, 46]
Step 4301/10000 - loss: 0.00244272 -aux_loss: 0.00005455-rank: 10,[28, 36, 46]
Step 4401/10000 - loss: 0.00245400 -aux_loss: 0.00021833-rank: 10,[28, 36, 46]
Step 4501/10000 - loss: 0.00148555 -aux_loss: 0.00002712-rank: 10,[27, 36, 46]
Step 4601/10000 - loss: 0.00235447 -aux_loss: 0.00002350-rank: 10,[27, 36, 46]
Step 4701/10000 - loss: 0.00105569 -aux_loss: 0.00004228-rank: 10,[27, 36, 46]
Step 4801/10000 - loss: 0.00225724 -aux_loss: 0.00003942-rank: 10,[27, 36, 46]
Step 4901/10000 - loss: 0.00667916 -aux_loss: 0.00009338-rank: 10,[26, 36, 46]
Step 5001/10000 - loss: 0.00288652 -aux_loss: 0.00005262-rank: 10,[27, 36, 46]
Step 5101/10000 - loss: 0.00096815 -aux_loss: 0.00001904-rank: 10,[27, 36, 46]
Step 5201/10000 - loss: 0.00107442 -aux_loss: 0.00003959-rank: 10,[27, 36, 46]
Step 5301/10000 - loss: 0.00091938 -aux_loss: 0.00006784-rank: 10,[27, 36, 46]
Step 5401/10000 - loss: 0.00258305 -aux_loss: 0.00000668-rank: 10,[26, 36, 46]
Step 5501/10000 - loss: 0.00181565 -aux_loss: 0.00004971-rank: 10,[26, 36, 46]
Step 5601/10000 - loss: 0.00234857 -aux_loss: 0.00002685-rank: 10,[26, 36, 46]
Step 5701/10000 - loss: 0.00107319 -aux_loss: 0.00003893-rank: 10,[26, 35, 46]
Step 5801/10000 - loss: 0.00174860 -aux_loss: 0.00006346-rank: 10,[26, 36, 46]
Step 5901/10000 - loss: 0.00393837 -aux_loss: 0.00010086-rank: 10,[25, 35, 45]
Step 6001/10000 - loss: 0.00142879 -aux_loss: 0.00002190-rank: 10,[25, 35, 45]
Step 6101/10000 - loss: 0.00213645 -aux_loss: 0.00001210-rank: 10,[25, 36, 45]
Step 6201/10000 - loss: 0.00125441 -aux_loss: 0.00000318-rank: 10,[25, 36, 44]
Step 6301/10000 - loss: 0.00259832 -aux_loss: 0.00009931-rank: 10,[26, 35, 44]
Step 6401/10000 - loss: 0.00120076 -aux_loss: 0.00003737-rank: 10,[25, 35, 44]
Step 6501/10000 - loss: 0.00125402 -aux_loss: 0.00001846-rank: 10,[25, 35, 44]
Step 6601/10000 - loss: 0.00142751 -aux_loss: 0.00000546-rank: 10,[25, 35, 44]
Step 6701/10000 - loss: 0.00121775 -aux_loss: 0.00009126-rank: 10,[25, 35, 44]
Step 6801/10000 - loss: 0.00174209 -aux_loss: 0.00004989-rank: 10,[25, 35, 44]
Step 6901/10000 - loss: 0.00120658 -aux_loss: 0.00008833-rank: 10,[25, 35, 44]
Step 7001/10000 - loss: 0.00101722 -aux_loss: 0.00002232-rank: 10,[25, 35, 44]
Step 7101/10000 - loss: 0.00063928 -aux_loss: 0.00004378-rank: 9,[25, 35, 44]
Step 7201/10000 - loss: 0.00238605 -aux_loss: 0.00006828-rank: 9,[25, 35, 44]
Step 7301/10000 - loss: 0.00191547 -aux_loss: 0.00002883-rank: 9,[25, 35, 44]
Step 7401/10000 - loss: 0.00159208 -aux_loss: 0.00003816-rank: 9,[25, 35, 44]
Step 7501/10000 - loss: 0.00158281 -aux_loss: 0.00000687-rank: 10,[25, 35, 44]
Step 7601/10000 - loss: 0.00068875 -aux_loss: 0.00012702-rank: 9,[25, 35, 44]
Step 7701/10000 - loss: 0.00325271 -aux_loss: 0.00005683-rank: 9,[25, 35, 44]
Step 7801/10000 - loss: 0.00155399 -aux_loss: 0.00002814-rank: 9,[25, 35, 44]
Step 7901/10000 - loss: 0.00447547 -aux_loss: 0.00008345-rank: 9,[25, 35, 44]
Step 8001/10000 - loss: 0.00183956 -aux_loss: 0.00019764-rank: 9,[25, 35, 44]
Step 8101/10000 - loss: 0.00101047 -aux_loss: 0.00006588-rank: 9,[25, 35, 44]
Step 8201/10000 - loss: 0.00173101 -aux_loss: 0.00005121-rank: 9,[25, 35, 44]
Step 8301/10000 - loss: 0.00160640 -aux_loss: 0.00019433-rank: 9,[25, 35, 44]
Step 8401/10000 - loss: 0.00158435 -aux_loss: 0.00003958-rank: 9,[25, 35, 44]
Step 8501/10000 - loss: 0.00059939 -aux_loss: 0.00014679-rank: 9,[25, 35, 44]
Step 8601/10000 - loss: 0.00127636 -aux_loss: 0.00000244-rank: 8,[24, 35, 44]
Step 8701/10000 - loss: 0.00123121 -aux_loss: 0.00001645-rank: 9,[25, 35, 44]
Step 8801/10000 - loss: 0.00054840 -aux_loss: 0.00014255-rank: 9,[25, 35, 44]
Step 8901/10000 - loss: 0.00190499 -aux_loss: 0.00001155-rank: 9,[25, 35, 44]
Step 9001/10000 - loss: 0.00038373 -aux_loss: 0.00003954-rank: 9,[25, 35, 44]
Step 9101/10000 - loss: 0.00109418 -aux_loss: 0.00008647-rank: 9,[25, 35, 44]
Step 9201/10000 - loss: 0.00267314 -aux_loss: 0.00004403-rank: 8,[24, 35, 44]
Step 9301/10000 - loss: 0.00088709 -aux_loss: 0.00000460-rank: 9,[25, 35, 44]
Step 9401/10000 - loss: 0.00245929 -aux_loss: 0.00003916-rank: 9,[25, 35, 43]
Step 9501/10000 - loss: 0.00058343 -aux_loss: 0.00001119-rank: 8,[23, 35, 44]
Step 9601/10000 - loss: 0.00045718 -aux_loss: 0.00002250-rank: 9,[24, 35, 43]
Step 9701/10000 - loss: 0.00092805 -aux_loss: 0.00005794-rank: 9,[24, 35, 43]
Step 9801/10000 - loss: 0.00149544 -aux_loss: 0.00003073-rank: 8,[23, 35, 43]
Step 9901/10000 - loss: 0.00033037 -aux_loss: 0.00000312-rank: 8,[23, 35, 43]
Step 10000/10000 - loss: 0.00050115 -aux_loss: 0.00001912-rank: 9,[24, 35, 43]
[2025-06-19 03:40:58] INFO: 【完成】train_loop，耗时：151.9025 秒
MoE_Model Evaluation Results - loss: 0.00077615, aux_loss: 0.00016847
Gates:
 tensor([[0.0000, 0.5206, 0.0000, 0.4794],
        [0.0000, 0.5241, 0.0000, 0.4759],
        [0.4983, 0.0000, 0.5017, 0.0000],
        [0.4986, 0.0000, 0.5014, 0.0000],
        [0.0000, 0.5935, 0.0000, 0.4065],
        [0.4955, 0.0000, 0.5045, 0.0000],
        [0.4996, 0.0000, 0.5004, 0.0000],
        [0.4998, 0.0000, 0.5002, 0.0000],
        [0.4997, 0.0000, 0.5003, 0.0000],
        [0.0000, 0.5751, 0.0000, 0.4249],
        [0.0000, 0.5266, 0.0000, 0.4734],
        [0.4977, 0.0000, 0.5023, 0.0000],
        [0.0000, 0.5313, 0.0000, 0.4687],
        [0.4987, 0.0000, 0.5013, 0.0000],
        [0.0000, 0.5049, 0.0000, 0.4951],
        [0.0000, 0.5334, 0.0000, 0.4666],
        [0.4988, 0.0000, 0.5012, 0.0000],
        [0.4969, 0.0000, 0.5031, 0.0000],
        [0.4991, 0.0000, 0.5009, 0.0000],
        [0.0000, 0.5705, 0.0000, 0.4295],
        [0.0000, 0.5393, 0.0000, 0.4607],
        [0.4981, 0.0000, 0.5019, 0.0000],
        [0.0000, 0.5005, 0.0000, 0.4995],
        [0.4964, 0.0000, 0.5036, 0.0000],
        [0.4977, 0.0000, 0.5023, 0.0000],
        [0.0000, 0.5379, 0.0000, 0.4621],
        [0.4983, 0.0000, 0.5017, 0.0000],
        [0.4983, 0.0000, 0.5017, 0.0000],
        [0.4989, 0.0000, 0.5011, 0.0000],
        [0.0000, 0.5102, 0.0000, 0.4897],
        [0.0000, 0.5746, 0.0000, 0.4254],
        [0.0000, 0.5340, 0.0000, 0.4660],
        [0.0000, 0.5655, 0.0000, 0.4345],
        [0.4995, 0.0000, 0.5005, 0.0000],
        [0.4955, 0.0000, 0.5045, 0.0000],
        [0.4976, 0.0000, 0.5024, 0.0000],
        [0.4991, 0.0000, 0.5009, 0.0000],
        [0.4990, 0.0000, 0.5010, 0.0000],
        [0.0000, 0.5017, 0.0000, 0.4983],
        [0.4971, 0.0000, 0.5029, 0.0000],
        [0.4986, 0.0000, 0.5013, 0.0000],
        [0.0000, 0.5313, 0.0000, 0.4687],
        [0.4966, 0.0000, 0.5034, 0.0000],
        [0.4957, 0.0000, 0.5043, 0.0000],
        [0.4953, 0.0000, 0.5047, 0.0000],
        [0.0000, 0.5031, 0.0000, 0.4969],
        [0.4969, 0.0000, 0.5031, 0.0000],
        [0.0000, 0.5766, 0.0000, 0.4234],
        [0.4959, 0.0000, 0.5041, 0.0000],
        [0.4989, 0.0000, 0.5011, 0.0000],
        [0.4957, 0.0000, 0.5043, 0.0000],
        [0.0000, 0.5711, 0.0000, 0.4289],
        [0.4985, 0.0000, 0.5015, 0.0000],
        [0.0000, 0.5633, 0.0000, 0.4367],
        [0.0000, 0.5176, 0.0000, 0.4824],
        [0.4969, 0.0000, 0.5031, 0.0000],
        [0.4960, 0.0000, 0.5040, 0.0000],
        [0.4980, 0.0000, 0.5020, 0.0000],
        [0.4972, 0.0000, 0.5028, 0.0000],
        [0.4998, 0.0000, 0.5002, 0.0000],
        [0.4963, 0.0000, 0.5037, 0.0000],
        [0.4955, 0.0000, 0.5045, 0.0000],
        [0.4978, 0.0000, 0.5022, 0.0000],
        [0.0000, 0.5230, 0.0000, 0.4770],
        [0.4983, 0.0000, 0.5017, 0.0000],
        [0.4959, 0.0000, 0.5041, 0.0000],
        [0.0000, 0.5520, 0.0000, 0.4480],
        [0.4971, 0.0000, 0.5029, 0.0000],
        [0.4998, 0.0000, 0.5002, 0.0000],
        [0.0000, 0.5833, 0.0000, 0.4167],
        [0.4998, 0.0000, 0.5002, 0.0000],
        [0.0000, 0.5637, 0.0000, 0.4363],
        [0.0000, 0.5764, 0.0000, 0.4236],
        [0.4992, 0.0000, 0.5008, 0.0000],
        [0.4970, 0.0000, 0.5030, 0.0000],
        [0.0000, 0.5065, 0.0000, 0.4935],
        [0.4953, 0.0000, 0.5047, 0.0000],
        [0.4979, 0.0000, 0.5021, 0.0000],
        [0.4995, 0.0000, 0.5005, 0.0000],
        [0.4996, 0.0000, 0.5004, 0.0000],
        [0.4958, 0.0000, 0.5042, 0.0000],
        [0.4976, 0.0000, 0.5024, 0.0000],
        [0.0000, 0.5692, 0.0000, 0.4308],
        [0.0000, 0.5232, 0.0000, 0.4768],
        [0.5000, 0.0000, 0.5000, 0.0000],
        [0.4985, 0.0000, 0.5015, 0.0000],
        [0.0000, 0.5454, 0.0000, 0.4546],
        [0.4969, 0.0000, 0.5031, 0.0000],
        [0.4953, 0.0000, 0.5047, 0.0000],
        [0.0000, 0.5936, 0.0000, 0.4064],
        [0.0000, 0.5642, 0.0000, 0.4358],
        [0.4967, 0.0000, 0.5033, 0.0000],
        [0.0000, 0.5922, 0.0000, 0.4078],
        [0.4976, 0.0000, 0.5024, 0.0000],
        [0.4980, 0.0000, 0.5020, 0.0000],
        [0.0000, 0.5797, 0.0000, 0.4203],
        [0.0000, 0.5577, 0.0000, 0.4423],
        [0.4965, 0.0000, 0.5035, 0.0000],
        [0.4988, 0.0000, 0.5012, 0.0000],
        [0.4993, 0.0000, 0.5007, 0.0000],
        [0.4983, 0.0000, 0.5017, 0.0000],
        [0.4980, 0.0000, 0.5020, 0.0000],
        [0.0000, 0.5523, 0.0000, 0.4477],
        [0.4974, 0.0000, 0.5026, 0.0000],
        [0.0000, 0.5252, 0.0000, 0.4748],
        [0.0000, 0.5860, 0.0000, 0.4140],
        [0.4986, 0.0000, 0.5014, 0.0000],
        [0.0000, 0.5854, 0.0000, 0.4146],
        [0.4996, 0.0000, 0.5004, 0.0000],
        [0.0000, 0.5909, 0.0000, 0.4091],
        [0.4991, 0.0000, 0.5009, 0.0000],
        [0.0000, 0.5884, 0.0000, 0.4116],
        [0.4996, 0.0000, 0.5004, 0.0000],
        [0.4959, 0.0000, 0.5041, 0.0000],
        [0.0000, 0.5417, 0.0000, 0.4583],
        [0.4956, 0.0000, 0.5044, 0.0000],
        [0.4960, 0.0000, 0.5040, 0.0000],
        [0.0000, 0.5478, 0.0000, 0.4522],
        [0.4975, 0.0000, 0.5025, 0.0000],
        [0.0000, 0.5223, 0.0000, 0.4777],
        [0.4959, 0.0000, 0.5041, 0.0000],
        [0.0000, 0.5315, 0.0000, 0.4685],
        [0.0000, 0.5780, 0.0000, 0.4220],
        [0.4996, 0.0000, 0.5004, 0.0000],
        [0.0000, 0.5752, 0.0000, 0.4248],
        [0.0000, 0.5233, 0.0000, 0.4767],
        [0.0000, 0.5467, 0.0000, 0.4533],
        [0.0000, 0.5958, 0.0000, 0.4042],
        [0.4977, 0.0000, 0.5023, 0.0000],
        [0.0000, 0.5765, 0.0000, 0.4235],
        [0.4984, 0.0000, 0.5016, 0.0000],
        [0.0000, 0.5590, 0.0000, 0.4410],
        [0.4976, 0.0000, 0.5024, 0.0000],
        [0.0000, 0.5863, 0.0000, 0.4137],
        [0.4968, 0.0000, 0.5032, 0.0000],
        [0.4965, 0.0000, 0.5035, 0.0000],
        [0.4988, 0.0000, 0.5012, 0.0000],
        [0.0000, 0.5881, 0.0000, 0.4119],
        [0.4977, 0.0000, 0.5023, 0.0000],
        [0.4990, 0.0000, 0.5010, 0.0000],
        [0.0000, 0.5932, 0.0000, 0.4068],
        [0.0000, 0.5228, 0.0000, 0.4772],
        [0.4980, 0.0000, 0.5020, 0.0000],
        [0.4954, 0.0000, 0.5046, 0.0000],
        [0.0000, 0.5729, 0.0000, 0.4271],
        [0.0000, 0.5565, 0.0000, 0.4435],
        [0.0000, 0.5720, 0.0000, 0.4280],
        [0.0000, 0.5501, 0.0000, 0.4499],
        [0.4988, 0.0000, 0.5012, 0.0000],
        [0.4988, 0.0000, 0.5012, 0.0000],
        [0.0000, 0.5500, 0.0000, 0.4500],
        [0.0000, 0.5012, 0.0000, 0.4988],
        [0.0000, 0.5072, 0.0000, 0.4928],
        [0.0000, 0.5614, 0.0000, 0.4386],
        [0.4997, 0.0000, 0.5003, 0.0000],
        [0.4997, 0.0000, 0.5003, 0.0000],
        [0.4959, 0.0000, 0.5041, 0.0000],
        [0.0000, 0.5852, 0.0000, 0.4148],
        [0.4962, 0.0000, 0.5038, 0.0000],
        [0.4960, 0.0000, 0.5040, 0.0000],
        [0.0000, 0.5285, 0.0000, 0.4715],
        [0.0000, 0.5536, 0.0000, 0.4464],
        [0.0000, 0.5514, 0.0000, 0.4486],
        [0.4970, 0.0000, 0.5030, 0.0000],
        [0.0000, 0.5640, 0.0000, 0.4360],
        [0.0000, 0.5064, 0.0000, 0.4936],
        [0.0000, 0.5680, 0.0000, 0.4320],
        [0.0000, 0.5382, 0.0000, 0.4618],
        [0.0000, 0.5728, 0.0000, 0.4272],
        [0.4979, 0.0000, 0.5021, 0.0000],
        [0.4972, 0.0000, 0.5028, 0.0000],
        [0.0000, 0.5402, 0.0000, 0.4598],
        [0.4980, 0.0000, 0.5020, 0.0000],
        [0.4996, 0.0000, 0.5004, 0.0000],
        [0.0000, 0.5344, 0.0000, 0.4656],
        [0.0000, 0.5462, 0.0000, 0.4538],
        [0.0000, 0.5274, 0.0000, 0.4726],
        [0.0000, 0.5869, 0.0000, 0.4131],
        [0.0000, 0.5385, 0.0000, 0.4615],
        [0.4997, 0.0000, 0.5003, 0.0000],
        [0.0000, 0.5632, 0.0000, 0.4368],
        [0.4995, 0.0000, 0.5005, 0.0000],
        [0.0000, 0.5048, 0.0000, 0.4952],
        [0.0000, 0.5864, 0.0000, 0.4136],
        [0.4954, 0.0000, 0.5046, 0.0000],
        [0.0000, 0.5024, 0.0000, 0.4976],
        [0.0000, 0.5643, 0.0000, 0.4357],
        [0.4985, 0.0000, 0.5015, 0.0000],
        [0.4991, 0.0000, 0.5009, 0.0000],
        [0.0000, 0.5512, 0.0000, 0.4488],
        [0.0000, 0.5624, 0.0000, 0.4376],
        [0.4981, 0.0000, 0.5019, 0.0000],
        [0.0000, 0.5875, 0.0000, 0.4125],
        [0.0000, 0.5085, 0.0000, 0.4915],
        [0.4995, 0.0000, 0.5005, 0.0000],
        [0.4962, 0.0000, 0.5038, 0.0000],
        [0.4986, 0.0000, 0.5014, 0.0000],
        [0.0000, 0.5601, 0.0000, 0.4399],
        [0.4959, 0.0000, 0.5041, 0.0000],
        [0.0000, 0.5110, 0.0000, 0.4890],
        [0.0000, 0.5285, 0.0000, 0.4715],
        [0.0000, 0.5682, 0.0000, 0.4318],
        [0.0000, 0.5506, 0.0000, 0.4494],
        [0.0000, 0.5132, 0.0000, 0.4868],
        [0.4957, 0.0000, 0.5043, 0.0000],
        [0.4993, 0.0000, 0.5007, 0.0000],
        [0.4968, 0.0000, 0.5032, 0.0000],
        [0.0000, 0.5787, 0.0000, 0.4213],
        [0.0000, 0.5366, 0.0000, 0.4634],
        [0.4954, 0.0000, 0.5046, 0.0000],
        [0.4999, 0.0000, 0.5001, 0.0000],
        [0.4976, 0.0000, 0.5024, 0.0000],
        [0.0000, 0.5370, 0.0000, 0.4630],
        [0.4963, 0.0000, 0.5037, 0.0000],
        [0.0000, 0.5737, 0.0000, 0.4263],
        [0.4989, 0.0000, 0.5011, 0.0000],
        [0.4981, 0.0000, 0.5019, 0.0000],
        [0.0000, 0.5733, 0.0000, 0.4267],
        [0.0000, 0.5578, 0.0000, 0.4422],
        [0.4963, 0.0000, 0.5037, 0.0000],
        [0.0000, 0.5297, 0.0000, 0.4703],
        [0.4997, 0.0000, 0.5003, 0.0000],
        [0.4979, 0.0000, 0.5021, 0.0000],
        [0.4960, 0.0000, 0.5040, 0.0000],
        [0.4994, 0.0000, 0.5006, 0.0000],
        [0.0000, 0.5184, 0.0000, 0.4816],
        [0.4976, 0.0000, 0.5024, 0.0000],
        [0.0000, 0.5144, 0.0000, 0.4856],
        [0.4977, 0.0000, 0.5023, 0.0000],
        [0.4983, 0.0000, 0.5017, 0.0000],
        [0.4964, 0.0000, 0.5036, 0.0000],
        [0.0000, 0.5805, 0.0000, 0.4195],
        [0.4973, 0.0000, 0.5027, 0.0000],
        [0.4994, 0.0000, 0.5006, 0.0000],
        [0.4991, 0.0000, 0.5009, 0.0000],
        [0.0000, 0.5058, 0.0000, 0.4942],
        [0.0000, 0.5583, 0.0000, 0.4417],
        [0.0000, 0.5475, 0.0000, 0.4525],
        [0.0000, 0.5375, 0.0000, 0.4625],
        [0.4960, 0.0000, 0.5040, 0.0000],
        [0.0000, 0.5407, 0.0000, 0.4593],
        [0.4962, 0.0000, 0.5038, 0.0000],
        [0.0000, 0.5088, 0.0000, 0.4912],
        [0.4964, 0.0000, 0.5036, 0.0000],
        [0.0000, 0.5605, 0.0000, 0.4395],
        [0.0000, 0.5838, 0.0000, 0.4162],
        [0.4979, 0.0000, 0.5021, 0.0000],
        [0.0000, 0.5889, 0.0000, 0.4111],
        [0.0000, 0.5598, 0.0000, 0.4402],
        [0.0000, 0.5635, 0.0000, 0.4365]], device='cuda:7',
       grad_fn=<ScatterBackward0>)
[2025-06-19 03:40:58] INFO: 【开始】train_loop
Step 1/10000 - loss: 2.46228245 -rank: [2, 4, 4, 5, 5]
Step 101/10000 - loss: 0.54045205 -rank: [2, 4, 5, 6, 8]
Step 201/10000 - loss: 0.49379987 -rank: [2, 4, 5, 6, 8]
Step 301/10000 - loss: 0.48635055 -rank: [2, 4, 5, 6, 9]
Step 401/10000 - loss: 0.48272250 -rank: [2, 4, 5, 7, 10]
Step 501/10000 - loss: 0.47166697 -rank: [2, 4, 6, 8, 14]
Step 601/10000 - loss: 0.43625702 -rank: [2, 4, 6, 10, 17]
Step 701/10000 - loss: 0.41765351 -rank: [2, 5, 6, 11, 22]
Step 801/10000 - loss: 0.39393841 -rank: [2, 5, 6, 13, 26]
Step 901/10000 - loss: 0.31730393 -rank: [2, 5, 7, 17, 30]
Step 1001/10000 - loss: 0.23711180 -rank: [2, 5, 8, 21, 34]
Step 1101/10000 - loss: 0.21220043 -rank: [2, 5, 8, 24, 35]
Step 1201/10000 - loss: 0.19092993 -rank: [2, 5, 8, 25, 35]
Step 1301/10000 - loss: 0.17531172 -rank: [2, 5, 9, 27, 35]
Step 1401/10000 - loss: 0.16154070 -rank: [2, 5, 10, 28, 35]
Step 1501/10000 - loss: 0.15267756 -rank: [2, 5, 11, 28, 35]
Step 1601/10000 - loss: 0.13435423 -rank: [2, 5, 11, 28, 37]
Step 1701/10000 - loss: 0.12662480 -rank: [2, 5, 12, 30, 37]
Step 1801/10000 - loss: 0.11248046 -rank: [2, 5, 13, 30, 39]
Step 1901/10000 - loss: 0.09926146 -rank: [2, 5, 13, 31, 39]
Step 2001/10000 - loss: 0.08858751 -rank: [2, 5, 14, 32, 39]
Step 2101/10000 - loss: 0.08256172 -rank: [2, 5, 14, 31, 39]
Step 2201/10000 - loss: 0.08070254 -rank: [2, 5, 14, 31, 37]
Step 2301/10000 - loss: 0.07939000 -rank: [2, 5, 14, 30, 38]
Step 2401/10000 - loss: 0.07510333 -rank: [2, 5, 14, 30, 38]
Step 2501/10000 - loss: 0.06342727 -rank: [2, 5, 14, 30, 38]
Step 2601/10000 - loss: 0.03592425 -rank: [2, 5, 15, 31, 39]
Step 2701/10000 - loss: 0.00715270 -rank: [2, 6, 15, 30, 40]
Step 2801/10000 - loss: 0.00338489 -rank: [2, 6, 15, 30, 40]
Step 2901/10000 - loss: 0.00065405 -rank: [2, 6, 14, 30, 39]
Step 3001/10000 - loss: 0.00284212 -rank: [2, 6, 14, 30, 39]
Step 3101/10000 - loss: 0.00173737 -rank: [2, 6, 14, 30, 40]
Step 3201/10000 - loss: 0.00072856 -rank: [2, 6, 14, 30, 40]
Step 3301/10000 - loss: 0.00329561 -rank: [2, 6, 14, 30, 39]
Step 3401/10000 - loss: 0.00129474 -rank: [2, 6, 14, 30, 39]
Step 3501/10000 - loss: 0.00035045 -rank: [2, 6, 13, 30, 39]
Step 3601/10000 - loss: 0.00253094 -rank: [2, 6, 13, 29, 39]
Step 3701/10000 - loss: 0.00170821 -rank: [2, 6, 13, 29, 39]
Step 3801/10000 - loss: 0.00030523 -rank: [2, 6, 13, 29, 38]
Step 3901/10000 - loss: 0.00311179 -rank: [2, 6, 13, 29, 38]
Step 4001/10000 - loss: 0.00183104 -rank: [2, 6, 13, 29, 39]
Step 4101/10000 - loss: 0.00021365 -rank: [2, 6, 12, 29, 39]
Step 4201/10000 - loss: 0.00231502 -rank: [2, 6, 12, 29, 39]
Step 4301/10000 - loss: 0.00162368 -rank: [2, 6, 12, 29, 38]
Step 4401/10000 - loss: 0.00028335 -rank: [2, 6, 12, 29, 38]
Step 4501/10000 - loss: 0.00307700 -rank: [2, 6, 12, 29, 38]
Step 4601/10000 - loss: 0.00123883 -rank: [2, 6, 12, 29, 38]
Step 4701/10000 - loss: 0.00050922 -rank: [2, 6, 12, 29, 38]
Step 4801/10000 - loss: 0.00277652 -rank: [2, 6, 12, 29, 38]
Step 4901/10000 - loss: 0.00054747 -rank: [2, 6, 12, 29, 38]
Step 5001/10000 - loss: 0.00188078 -rank: [2, 6, 12, 29, 38]
Step 5101/10000 - loss: 0.00228297 -rank: [2, 6, 12, 29, 37]
Step 5201/10000 - loss: 0.00022416 -rank: [2, 6, 12, 29, 37]
Step 5301/10000 - loss: 0.00248989 -rank: [2, 6, 12, 28, 37]
Step 5401/10000 - loss: 0.00122605 -rank: [2, 6, 12, 28, 37]
Step 5501/10000 - loss: 0.00102608 -rank: [2, 6, 12, 28, 37]
Step 5601/10000 - loss: 0.00252186 -rank: [2, 6, 12, 28, 37]
Step 5701/10000 - loss: 0.00030462 -rank: [2, 6, 12, 28, 37]
Step 5801/10000 - loss: 0.00172351 -rank: [2, 6, 12, 28, 37]
Step 5901/10000 - loss: 0.00194021 -rank: [2, 6, 12, 28, 37]
Step 6001/10000 - loss: 0.00025007 -rank: [2, 6, 12, 28, 37]
Step 6101/10000 - loss: 0.00297095 -rank: [2, 6, 11, 27, 37]
Step 6201/10000 - loss: 0.00135314 -rank: [2, 6, 11, 27, 37]
Step 6301/10000 - loss: 0.00040803 -rank: [2, 6, 11, 27, 37]
Step 6401/10000 - loss: 0.00253494 -rank: [2, 6, 11, 27, 37]
Step 6501/10000 - loss: 0.00075088 -rank: [2, 6, 11, 27, 37]
Step 6601/10000 - loss: 0.00180936 -rank: [2, 6, 11, 27, 37]
Step 6701/10000 - loss: 0.00225194 -rank: [2, 6, 11, 27, 37]
Step 6801/10000 - loss: 0.00021079 -rank: [2, 6, 11, 27, 37]
Step 6901/10000 - loss: 0.00215788 -rank: [2, 6, 11, 27, 37]
Step 7001/10000 - loss: 0.00127680 -rank: [2, 6, 11, 27, 37]
Step 7101/10000 - loss: 0.00088041 -rank: [2, 6, 11, 27, 37]
Step 7201/10000 - loss: 0.00269401 -rank: [2, 6, 11, 27, 37]
Step 7301/10000 - loss: 0.00053235 -rank: [2, 6, 11, 27, 38]
Step 7401/10000 - loss: 0.00176057 -rank: [2, 6, 11, 27, 38]
Step 7501/10000 - loss: 0.00166138 -rank: [2, 6, 11, 27, 38]
Step 7601/10000 - loss: 0.00036536 -rank: [2, 6, 11, 27, 38]
Step 7701/10000 - loss: 0.00272000 -rank: [2, 6, 11, 27, 38]
Step 7801/10000 - loss: 0.00080571 -rank: [2, 6, 11, 27, 38]
Step 7901/10000 - loss: 0.00109171 -rank: [2, 6, 11, 27, 38]
Step 8001/10000 - loss: 0.00203405 -rank: [2, 6, 11, 27, 38]
Step 8101/10000 - loss: 0.00012875 -rank: [2, 6, 11, 27, 38]
Step 8201/10000 - loss: 0.00259299 -rank: [2, 6, 11, 27, 38]
Step 8301/10000 - loss: 0.00110443 -rank: [2, 6, 11, 27, 38]
Step 8401/10000 - loss: 0.00052368 -rank: [2, 6, 11, 27, 38]
Step 8501/10000 - loss: 0.00213105 -rank: [2, 6, 11, 27, 38]
Step 8601/10000 - loss: 0.00037951 -rank: [2, 6, 11, 27, 38]
Step 8701/10000 - loss: 0.00254218 -rank: [2, 6, 11, 27, 38]
Step 8801/10000 - loss: 0.00044241 -rank: [2, 6, 11, 27, 38]
Step 8901/10000 - loss: 0.00131398 -rank: [2, 6, 11, 27, 38]
Step 9001/10000 - loss: 0.00168661 -rank: [2, 6, 11, 27, 38]
Step 9101/10000 - loss: 0.00020680 -rank: [2, 6, 11, 27, 38]
Step 9201/10000 - loss: 0.00253773 -rank: [2, 6, 11, 27, 38]
Step 9301/10000 - loss: 0.00034016 -rank: [2, 6, 11, 27, 38]
Step 9401/10000 - loss: 0.00168218 -rank: [2, 6, 11, 27, 38]
Step 9501/10000 - loss: 0.00102893 -rank: [2, 6, 11, 27, 38]
Step 9601/10000 - loss: 0.00114697 -rank: [2, 6, 11, 26, 38]
Step 9701/10000 - loss: 0.00224565 -rank: [2, 6, 11, 26, 38]
Step 9801/10000 - loss: 0.00026527 -rank: [2, 6, 11, 26, 38]
Step 9901/10000 - loss: 0.00146655 -rank: [2, 6, 11, 26, 38]
Step 10000/10000 - loss: 0.00077538 -rank: [2, 6, 11, 26, 38]
[2025-06-19 03:41:43] INFO: 【完成】train_loop，耗时：44.9085 秒
MLP_Model Evaluation Results - loss: 0.00136138
(env-zqh) 